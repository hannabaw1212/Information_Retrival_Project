{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c073778e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hanna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet') # Download WordNet if you haven't already\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b786e",
   "metadata": {},
   "source": [
    "# helping methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db07746c",
   "metadata": {},
   "source": [
    "## creating the unigram model and calculating the term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f304ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unig(tokens):\n",
    "    token_dict = {}\n",
    "\n",
    "# Iterate over the list of tokens\n",
    "    for token in tokens:\n",
    "    # Check if the token is already in the dictionary\n",
    "        if token in token_dict:\n",
    "        # If it is, increment its value by 1\n",
    "            token_dict[token] += 1\n",
    "        else:\n",
    "        # If it's not, add it to the dictionary with a value of 1\n",
    "            token_dict[token] = 1\n",
    "    return token_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbce36a",
   "metadata": {},
   "source": [
    "## function  to calculate the df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24339dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\" # reading our text files\n",
    "\n",
    "def df(dataF):\n",
    "    df_s = {}\n",
    "    list_of_docs = []\n",
    "    for doc in names:\n",
    "        with open('documents/' + doc, encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "#       df_s[token] = sum(token in content for document in documents)\n",
    "            list_of_docs.append(content)\n",
    "\n",
    "    for token in dataF[\"Token\"]:\n",
    "        df_s[token] = sum(token in document for document in list_of_docs)\n",
    "    \n",
    "    return df_s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d805217",
   "metadata": {},
   "source": [
    "# reading our corpus and do some pre-processing to have a clear tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "636c82bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about',\n",
       " 'above',\n",
       " 'abroad',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'adj',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ago',\n",
       " 'ahead',\n",
       " \"ain't\",\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amid',\n",
       " 'amidst',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'around',\n",
       " 'as',\n",
       " \"a's\",\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'associated',\n",
       " 'at',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'back',\n",
       " 'backward',\n",
       " 'backwards',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'begin',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'but',\n",
       " 'by',\n",
       " 'came',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " \"can't\",\n",
       " 'caption',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'changes',\n",
       " 'clearly',\n",
       " \"c'mon\",\n",
       " 'co',\n",
       " 'co.',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'concerning',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'corresponding',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'course',\n",
       " \"c's\",\n",
       " 'currently',\n",
       " 'dare',\n",
       " \"daren't\",\n",
       " 'definitely',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'different',\n",
       " 'directly',\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'done',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'during',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'eighty',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'end',\n",
       " 'ending',\n",
       " 'enough',\n",
       " 'entirely',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'evermore',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'fairly',\n",
       " 'far',\n",
       " 'farther',\n",
       " 'few',\n",
       " 'fewer',\n",
       " 'fifth',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'forever',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'greetings',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'half',\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " \"here's\",\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'hopefully',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'hundred',\n",
       " \"i'd\",\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ignored',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'immediate',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'inc.',\n",
       " 'indeed',\n",
       " 'indicate',\n",
       " 'indicated',\n",
       " 'indicates',\n",
       " 'inner',\n",
       " 'inside',\n",
       " 'insofar',\n",
       " 'instead',\n",
       " 'into',\n",
       " 'inward',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " 'its',\n",
       " \"it's\",\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " \"let's\",\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'likewise',\n",
       " 'little',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'low',\n",
       " 'lower',\n",
       " 'ltd',\n",
       " 'made',\n",
       " 'mainly',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " \"mayn't\",\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meantime',\n",
       " 'meanwhile',\n",
       " 'merely',\n",
       " 'might',\n",
       " \"mightn't\",\n",
       " 'mine',\n",
       " 'minus',\n",
       " 'miss',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'mr',\n",
       " 'mrs',\n",
       " 'much',\n",
       " 'must',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'need',\n",
       " \"needn't\",\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'neverf',\n",
       " 'neverless',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'ninety',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'nonetheless',\n",
       " 'noone',\n",
       " 'no-one',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'notwithstanding',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " \"one's\",\n",
       " 'only',\n",
       " 'onto',\n",
       " 'opposite',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " \"oughtn't\",\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'own',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'past',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'possible',\n",
       " 'presumably',\n",
       " 'probably',\n",
       " 'provided',\n",
       " 'provides',\n",
       " 'que',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 're',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'relatively',\n",
       " 'respectively',\n",
       " 'right',\n",
       " 'round',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'since',\n",
       " 'six',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'someday',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'still',\n",
       " 'sub',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'taking',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'th',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'thats',\n",
       " \"that's\",\n",
       " \"that've\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " \"there'd\",\n",
       " 'therefore',\n",
       " 'therein',\n",
       " \"there'll\",\n",
       " \"there're\",\n",
       " 'theres',\n",
       " \"there's\",\n",
       " 'thereupon',\n",
       " \"there've\",\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'third',\n",
       " 'thirty',\n",
       " 'this',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'till',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'tried',\n",
       " 'tries',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " \"t's\",\n",
       " 'twice',\n",
       " 'two',\n",
       " 'un',\n",
       " 'under',\n",
       " 'underneath',\n",
       " 'undoing',\n",
       " 'unfortunately',\n",
       " 'unless',\n",
       " 'unlike',\n",
       " 'unlikely',\n",
       " 'until',\n",
       " 'unto',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'upwards',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'v',\n",
       " 'value',\n",
       " 'various',\n",
       " 'versus',\n",
       " 'very',\n",
       " 'via',\n",
       " 'viz',\n",
       " 'vs',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'way',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " 'welcome',\n",
       " 'well',\n",
       " \"we'll\",\n",
       " 'went',\n",
       " 'were',\n",
       " \"we're\",\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'whatever',\n",
       " \"what'll\",\n",
       " \"what's\",\n",
       " \"what've\",\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " \"where's\",\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'whichever',\n",
       " 'while',\n",
       " 'whilst',\n",
       " 'whither',\n",
       " 'who',\n",
       " \"who'd\",\n",
       " 'whoever',\n",
       " 'whole',\n",
       " \"who'll\",\n",
       " 'whom',\n",
       " 'whomever',\n",
       " \"who's\",\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'willing',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'wonder',\n",
       " \"won't\",\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\",\n",
       " 'zero',\n",
       " 'a',\n",
       " \"how's\",\n",
       " 'i',\n",
       " \"when's\",\n",
       " \"why's\",\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'j',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'uucp',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'I',\n",
       " 'www',\n",
       " 'amount',\n",
       " 'bill',\n",
       " 'bottom',\n",
       " 'call',\n",
       " 'computer',\n",
       " 'con',\n",
       " 'couldnt',\n",
       " 'cry',\n",
       " 'de',\n",
       " 'describe',\n",
       " 'detail',\n",
       " 'due',\n",
       " 'eleven',\n",
       " 'empty',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'fill',\n",
       " 'find',\n",
       " 'fire',\n",
       " 'forty',\n",
       " 'front',\n",
       " 'full',\n",
       " 'give',\n",
       " 'hasnt',\n",
       " 'herse',\n",
       " 'himse',\n",
       " 'interest',\n",
       " 'itse”',\n",
       " 'mill',\n",
       " 'move',\n",
       " 'myse”',\n",
       " 'part',\n",
       " 'put',\n",
       " 'show',\n",
       " 'side',\n",
       " 'sincere',\n",
       " 'sixty',\n",
       " 'system',\n",
       " 'ten',\n",
       " 'thick',\n",
       " 'thin',\n",
       " 'top',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'abst',\n",
       " 'accordance',\n",
       " 'act',\n",
       " 'added',\n",
       " 'adopted',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affects',\n",
       " 'ah',\n",
       " 'announce',\n",
       " 'anymore',\n",
       " 'apparently',\n",
       " 'approximately',\n",
       " 'aren',\n",
       " 'arent',\n",
       " 'arise',\n",
       " 'auth',\n",
       " 'beginning',\n",
       " 'beginnings',\n",
       " 'begins',\n",
       " 'biol',\n",
       " 'briefly',\n",
       " 'ca',\n",
       " 'date',\n",
       " 'ed',\n",
       " 'effect',\n",
       " 'et-al',\n",
       " 'ff',\n",
       " 'fix',\n",
       " 'gave',\n",
       " 'giving',\n",
       " 'heres',\n",
       " 'hes',\n",
       " 'hid',\n",
       " 'home',\n",
       " 'id',\n",
       " 'im',\n",
       " 'immediately',\n",
       " 'importance',\n",
       " 'important',\n",
       " 'index',\n",
       " 'information',\n",
       " 'invention',\n",
       " 'itd',\n",
       " 'keys',\n",
       " 'kg',\n",
       " 'km',\n",
       " 'largely',\n",
       " 'lets',\n",
       " 'line',\n",
       " \"'ll\",\n",
       " 'means',\n",
       " 'mg',\n",
       " 'million',\n",
       " 'ml',\n",
       " 'mug',\n",
       " 'na',\n",
       " 'nay',\n",
       " 'necessarily',\n",
       " 'nos',\n",
       " 'noted',\n",
       " 'obtain',\n",
       " 'obtained',\n",
       " 'omitted',\n",
       " 'ord',\n",
       " 'owing',\n",
       " 'page',\n",
       " 'pages',\n",
       " 'poorly',\n",
       " 'possibly',\n",
       " 'potentially',\n",
       " 'pp',\n",
       " 'predominantly',\n",
       " 'present',\n",
       " 'previously',\n",
       " 'primarily',\n",
       " 'promptly',\n",
       " 'proud',\n",
       " 'quickly',\n",
       " 'ran',\n",
       " 'readily',\n",
       " 'ref',\n",
       " 'refs',\n",
       " 'related',\n",
       " 'research',\n",
       " 'resulted',\n",
       " 'resulting',\n",
       " 'results',\n",
       " 'run',\n",
       " 'sec',\n",
       " 'section',\n",
       " 'shed',\n",
       " 'shes',\n",
       " 'showed',\n",
       " 'shown',\n",
       " 'showns',\n",
       " 'shows',\n",
       " 'significant',\n",
       " 'significantly',\n",
       " 'similar',\n",
       " 'similarly',\n",
       " 'slightly',\n",
       " 'somethan',\n",
       " 'specifically',\n",
       " 'state',\n",
       " 'states',\n",
       " 'stop',\n",
       " 'strongly',\n",
       " 'substantially',\n",
       " 'successfully',\n",
       " 'sufficiently',\n",
       " 'suggest',\n",
       " 'thered',\n",
       " 'thereof',\n",
       " 'therere',\n",
       " 'thereto',\n",
       " 'theyd',\n",
       " 'theyre',\n",
       " 'thou',\n",
       " 'thoughh',\n",
       " 'thousand',\n",
       " 'throug',\n",
       " 'til',\n",
       " 'tip',\n",
       " 'ts',\n",
       " 'ups',\n",
       " 'usefully',\n",
       " 'usefulness',\n",
       " \"'ve\",\n",
       " 'vol',\n",
       " 'vols',\n",
       " 'wed',\n",
       " 'whats',\n",
       " 'wheres',\n",
       " 'whim',\n",
       " 'whod',\n",
       " 'whos',\n",
       " 'widely',\n",
       " 'words',\n",
       " 'world',\n",
       " 'youd',\n",
       " 'youre']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = pd.read_csv('stop_words_english.txt', delimiter=',')\n",
    "stop_words = list(stop_words['able'])\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3fa234",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.append('able')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "941c2f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A comparison of explanations given by explainable artificial intelligence methods on analysing electronic health records_.txt',\n",
       " 'A method to evaluate task specific importance of spatio temporal units based on explainable artificial intelligence.txt',\n",
       " 'A multi-component framework for the analysis and design of explainble AI.txt',\n",
       " 'A systematic review of explainable artificial intelligence in terms of different application domains and tasks_.txt',\n",
       " 'An_Explainable_Artificial_Intelligence_Model_for_Clustering_Numerical_Databases.txt',\n",
       " 'Applied AI Letters - 2021 - Yang - Abstraction  validation  and generalization for explainable artificial intelligence.txt',\n",
       " 'A_Survey_on_Explainable_Artificial_Intelligence_Techniques_and_Challenges.txt',\n",
       " 'A_Survey_on_Explainable_Artificial_Intelligence_XAI_Toward_Medical_XAI.txt',\n",
       " 'Black_Box_Attacks_on_Explainable_Artificial_IntelligenceXAI_methods_in_Cyber_Security.txt',\n",
       " 'Building Explainable Artificial Intelligence Systems.txt',\n",
       " 'Building More Explainable Artificial Intelligence with Argumentation.txt',\n",
       " 'DARPA’s Explainable.txt',\n",
       " 'Deep Opacity Undermines Data Protection and Explainable Artificial Intelligence_.txt',\n",
       " 'Ecography - 2020 - Ryo - Explainable artificial intelligence enhances the ecological interpretability of black‐box species.txt',\n",
       " 'Evolving_Rule-Based_Explainable_Artificial_Intelligence_for_Unmanned_Aerial_Vehicles.txt',\n",
       " 'Explainability Is in the Mind of the Beholder.txt',\n",
       " 'Explainable Artificial Intelligence (XAI)_.txt',\n",
       " 'Explainable artificial intelligence an analytical review.txt',\n",
       " 'Explainable Artificial Intelligence Applications in Cyber Security State-of-the-Art in Research_.txt',\n",
       " 'Explainable Artificial Intelligence Enabled TeleOphthalmology for Diabetic Retinopathy Grading and Classification_.txt',\n",
       " 'Explainable artificial intelligence for bias detection in covid ct-scan classifiers_.txt',\n",
       " 'Explainable artificial intelligence for developing smart cities solutions_.txt',\n",
       " 'Explainable Artificial Intelligence for Intrusion Detection System_.txt',\n",
       " 'Explainable Artificial Intelligence Helps in Understanding the Effect of Fibronectin on Survival of Sepsis_.txt',\n",
       " 'Explainable Artificial Intelligence in the Early Diagnosis of Gastrointestinal Disease_.txt',\n",
       " 'Explainable artificial intelligence model for diagnosis of atrial fibrillation using holter electrocardiogram waveforms_.txt',\n",
       " 'Explainable Artificial Intelligence Objectives.txt',\n",
       " 'Explainable Artificial Intelligence via Bayesian.txt',\n",
       " 'Explainable Artificial Intelligence, Lawyer’s Perspective.txt',\n",
       " 'Explainable-Artificial-Intelligence_-A-Survey-FINAL.txt',\n",
       " 'ExplainableArtificialIntelligence.txt',\n",
       " 'Explainable_Artificial_Intelligence_for_Data_Science_on_Customer_Churn.txt',\n",
       " 'Explainable_Machine_Learning_Exploiting_News_and_Domain-Specific_Lexicon_for_Stock_Market_Forecasting.txt',\n",
       " 'Exploring the Need for Explainable Artificial Intelligence.txt',\n",
       " 'Four Principles of Explainable Artificial Int.txt',\n",
       " 'From Zadeh’s Computing with Words Towards eXplainable Artificial Intelligence.txt',\n",
       " 'Interactive Natural Language Technology for Explainable Artificial Intelligence.txt',\n",
       " 'Interpretation of cluster structures in pain‐related phenotype data using explainable artificial intelligence (XAI)_.txt',\n",
       " 'Knowledge-to-information translation training (KITT) an adaptive approach to explainable artificial intelligence_.txt',\n",
       " 'PointHop_An_Explainable_Machine_Learning_Method_for_Point_Cloud_Classification.txt',\n",
       " 'Reasons, values, stakeholders A philosophical framework for explainable artificial intelligence_.txt',\n",
       " 'Roadmap of Designing Cognitive Metrics for Explainable AI.txt',\n",
       " 'Robust_Network_Intrusion_Detection_Through_Explainable_Artificial_Intelligence_XAI.txt',\n",
       " 'SeXAI a semantic explainable artificial intelligence framework_.txt',\n",
       " 'Towards Explainable Artificial Intelligence.txt',\n",
       " 'Using Explainable Artificial Intelligence to Improve.txt',\n",
       " 'Vibration Signals Analysis by Explainable AI.txt',\n",
       " 'WIREs Data Min   Knowl - 2020 - Confalonieri - A historical perspective of explainable Artificial Intelligence.txt',\n",
       " 'XAI4FL enhancing spectrum-based fault localization with explainable artificial intelligence_.txt',\n",
       " '[PDF] Feature importance of stabilised rammed earth components affecting the compressive strength calculated with explainable artificial intelligence tools_.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = os.listdir('documents/')\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db6b1354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# names.remove('.ipynb_checkpoints')\n",
    "# names.remove('Untitled.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dfbb942",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\" # reading our text files\n",
    "\n",
    "for doc in names:\n",
    "    with open('documents/' + doc, encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        corpus = corpus + str(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83458d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f34b807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3012321"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = re.sub('[^a-zA-Z]+', ' ', corpus) # i only want a alphabetacal words that good enough for our purpose\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4aee6bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461618"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = corpus.split(\" \")\n",
    "len(tokens)\n",
    "tokens = [s for s in tokens if len(s) > 1 or ((s =='i' or s == 'I') or (s == 'a' or s == 'A'))]\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a36f5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_tokens = [word for word in tokens if wordnet.synsets(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee9355d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316399"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(our_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556af49",
   "metadata": {},
   "source": [
    "## creating our simple dictionary by looking at the corpus in a unigram sight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "179d4abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unig(tokens):\n",
    "    token_dict = {}\n",
    "\n",
    "# Iterate over the list of tokens\n",
    "    for token in tokens:\n",
    "    # Check if the token is already in the dictionary\n",
    "        if token in token_dict:\n",
    "        # If it is, increment its value by 1\n",
    "            token_dict[token] += 1\n",
    "        else:\n",
    "        # If it's not, add it to the dictionary with a value of 1\n",
    "            token_dict[token] = 1\n",
    "    return token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea4af3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dict = create_unig(our_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ecf94f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = simple_dict.keys()\n",
    "l2 = simple_dict.values()\n",
    "temp_dict = {'Token' : l1, 'TF' : l2}\n",
    "our_tokens_df = pd.DataFrame(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "984de56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>TF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comparison</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explanations</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Given</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>by</td>\n",
       "      <td>2271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17553</th>\n",
       "      <td>Saint</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17554</th>\n",
       "      <td>Interlaken</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17555</th>\n",
       "      <td>Askes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17556</th>\n",
       "      <td>strain</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17557</th>\n",
       "      <td>Solids</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17558 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Token    TF\n",
       "0                 A  2461\n",
       "1        Comparison    32\n",
       "2      Explanations   217\n",
       "3             Given    39\n",
       "4                by  2271\n",
       "...             ...   ...\n",
       "17553         Saint     1\n",
       "17554    Interlaken     1\n",
       "17555         Askes     1\n",
       "17556        strain     1\n",
       "17557        Solids     1\n",
       "\n",
       "[17558 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_tokens_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32329e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_s = df(our_tokens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c088bffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_tokens_df[\"df\"] = df_s.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bd1bef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>TF</th>\n",
       "      <th>df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2461</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comparison</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explanations</td>\n",
       "      <td>217</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Given</td>\n",
       "      <td>39</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>by</td>\n",
       "      <td>2271</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17553</th>\n",
       "      <td>Saint</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17554</th>\n",
       "      <td>Interlaken</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17555</th>\n",
       "      <td>Askes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17556</th>\n",
       "      <td>strain</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17557</th>\n",
       "      <td>Solids</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17558 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Token    TF  df\n",
       "0                 A  2461  50\n",
       "1        Comparison    32  15\n",
       "2      Explanations   217  34\n",
       "3             Given    39  21\n",
       "4                by  2271  50\n",
       "...             ...   ...  ..\n",
       "17553         Saint     1   1\n",
       "17554    Interlaken     1   1\n",
       "17555         Askes     1   1\n",
       "17556        strain     1  28\n",
       "17557        Solids     1   1\n",
       "\n",
       "[17558 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_tokens_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e2c3533",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_tokens_df['idf'] = np.log10((50/our_tokens_df['df'] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e80ffd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>TF</th>\n",
       "      <th>df</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>on</td>\n",
       "      <td>3503</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2461</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>by</td>\n",
       "      <td>2271</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explainable</td>\n",
       "      <td>626</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Intelligence</td>\n",
       "      <td>549</td>\n",
       "      <td>48</td>\n",
       "      <td>0.017729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Artificial</td>\n",
       "      <td>537</td>\n",
       "      <td>49</td>\n",
       "      <td>0.008774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explanations</td>\n",
       "      <td>217</td>\n",
       "      <td>34</td>\n",
       "      <td>0.167491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Methods</td>\n",
       "      <td>116</td>\n",
       "      <td>36</td>\n",
       "      <td>0.142668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Given</td>\n",
       "      <td>39</td>\n",
       "      <td>21</td>\n",
       "      <td>0.376751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comparison</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>0.522879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Token    TF  df       idf\n",
       "9            on  3503  50  0.000000\n",
       "0             A  2461  50  0.000000\n",
       "4            by  2271  50  0.000000\n",
       "5   Explainable   626  50  0.000000\n",
       "7  Intelligence   549  48  0.017729\n",
       "6    Artificial   537  49  0.008774\n",
       "2  Explanations   217  34  0.167491\n",
       "8       Methods   116  36  0.142668\n",
       "3         Given    39  21  0.376751\n",
       "1    Comparison    32  15  0.522879"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_10 = our_tokens_df[0:10].sort_values(by=['TF'], ascending=False)\n",
    "token_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a3ca66",
   "metadata": {},
   "source": [
    "## Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ff2f919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'above', 'abroad', 'according', 'accordingly', 'across', 'actually', 'adj', 'after', 'afterwards', 'again', 'against', 'ago', 'ahead', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'alongside', 'already', 'also', 'although', 'always', 'am', 'amid', 'amidst', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as']\n"
     ]
    }
   ],
   "source": [
    "print(stop_words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16e7bc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226484"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_terms_after_SP = [token for token in our_tokens if token not in stop_words]\n",
    "len(our_terms_after_SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28b4fcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_dict = create_unig(our_terms_after_SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bba20224",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = stop_dict.keys()\n",
    "s2 = stop_dict.values()\n",
    "\n",
    "td = {\"Token\" : s1, \"TF\" : s2}\n",
    "stop_word_dicSTOPWORDS = pd.DataFrame(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a46fbb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>TF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comparison</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explanations</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Given</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explainable</td>\n",
       "      <td>626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17020</th>\n",
       "      <td>Saint</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17021</th>\n",
       "      <td>Interlaken</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17022</th>\n",
       "      <td>Askes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17023</th>\n",
       "      <td>strain</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17024</th>\n",
       "      <td>Solids</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17025 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Token    TF\n",
       "0                 A  2461\n",
       "1        Comparison    32\n",
       "2      Explanations   217\n",
       "3             Given    39\n",
       "4       Explainable   626\n",
       "...             ...   ...\n",
       "17020         Saint     1\n",
       "17021    Interlaken     1\n",
       "17022         Askes     1\n",
       "17023        strain     1\n",
       "17024        Solids     1\n",
       "\n",
       "[17025 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_word_dicSTOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "442fee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = df(stop_word_dicSTOPWORDS).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9483fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_dicSTOPWORDS['df'] = idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2717ed2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>TF</th>\n",
       "      <th>df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2461</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comparison</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explanations</td>\n",
       "      <td>217</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Given</td>\n",
       "      <td>39</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explainable</td>\n",
       "      <td>626</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17020</th>\n",
       "      <td>Saint</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17021</th>\n",
       "      <td>Interlaken</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17022</th>\n",
       "      <td>Askes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17023</th>\n",
       "      <td>strain</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17024</th>\n",
       "      <td>Solids</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17025 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Token    TF  df\n",
       "0                 A  2461  50\n",
       "1        Comparison    32  15\n",
       "2      Explanations   217  34\n",
       "3             Given    39  21\n",
       "4       Explainable   626  50\n",
       "...             ...   ...  ..\n",
       "17020         Saint     1   1\n",
       "17021    Interlaken     1   1\n",
       "17022         Askes     1   1\n",
       "17023        strain     1  28\n",
       "17024        Solids     1   1\n",
       "\n",
       "[17025 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_word_dicSTOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6604e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_dicSTOPWORDS['idf'] = np.log((50 / stop_word_dicSTOPWORDS['df']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f52d24d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>TF</th>\n",
       "      <th>df</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2461</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Comparison</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>1.203973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Explanations</td>\n",
       "      <td>217</td>\n",
       "      <td>34</td>\n",
       "      <td>0.385662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Given</td>\n",
       "      <td>39</td>\n",
       "      <td>21</td>\n",
       "      <td>0.867501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explainable</td>\n",
       "      <td>626</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17020</th>\n",
       "      <td>Saint</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.912023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17021</th>\n",
       "      <td>Interlaken</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.912023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17022</th>\n",
       "      <td>Askes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.912023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17023</th>\n",
       "      <td>strain</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.579818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17024</th>\n",
       "      <td>Solids</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.912023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17025 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Token    TF  df       idf\n",
       "0                 A  2461  50  0.000000\n",
       "1        Comparison    32  15  1.203973\n",
       "2      Explanations   217  34  0.385662\n",
       "3             Given    39  21  0.867501\n",
       "4       Explainable   626  50  0.000000\n",
       "...             ...   ...  ..       ...\n",
       "17020         Saint     1   1  3.912023\n",
       "17021    Interlaken     1   1  3.912023\n",
       "17022         Askes     1   1  3.912023\n",
       "17023        strain     1  28  0.579818\n",
       "17024        Solids     1   1  3.912023\n",
       "\n",
       "[17025 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_word_dicSTOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "484bdb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>TF</th>\n",
       "      <th>df</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>2461</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>model</td>\n",
       "      <td>2329</td>\n",
       "      <td>49</td>\n",
       "      <td>0.020203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>In</td>\n",
       "      <td>1811</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>data</td>\n",
       "      <td>1741</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>explanations</td>\n",
       "      <td>1494</td>\n",
       "      <td>45</td>\n",
       "      <td>0.105361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>explanation</td>\n",
       "      <td>1487</td>\n",
       "      <td>47</td>\n",
       "      <td>0.061875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>based</td>\n",
       "      <td>1373</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>models</td>\n",
       "      <td>1271</td>\n",
       "      <td>48</td>\n",
       "      <td>0.040822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>learning</td>\n",
       "      <td>1236</td>\n",
       "      <td>49</td>\n",
       "      <td>0.020203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>AI</td>\n",
       "      <td>1222</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Token    TF  df       idf\n",
       "0               A  2461  50  0.000000\n",
       "104         model  2329  49  0.020203\n",
       "54             In  1811  50  0.000000\n",
       "147          data  1741  50  0.000000\n",
       "34   explanations  1494  45  0.105361\n",
       "282   explanation  1487  47  0.061875\n",
       "262         based  1373  50  0.000000\n",
       "69         models  1271  48  0.040822\n",
       "42       learning  1236  49  0.020203\n",
       "85             AI  1222  50  0.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopW_10 = stop_word_dicSTOPWORDS.sort_values(by=['TF'], ascending=False)[0:10]\n",
    "stopW_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3daed6",
   "metadata": {},
   "source": [
    "## Case folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22e91038",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_folded_tokens = [token.lower() for token in our_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0ee59569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316399"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(case_folded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "017cee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "caseFolding_df = create_unig(case_folded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7db0459c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>TF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>10140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comparison</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>explanations</td>\n",
       "      <td>1730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>given</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>by</td>\n",
       "      <td>2360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12786</th>\n",
       "      <td>saint</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12787</th>\n",
       "      <td>interlaken</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12788</th>\n",
       "      <td>askes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12789</th>\n",
       "      <td>strain</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12790</th>\n",
       "      <td>solids</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12791 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Token     TF\n",
       "0                 a  10140\n",
       "1        comparison    129\n",
       "2      explanations   1730\n",
       "3             given    340\n",
       "4                by   2360\n",
       "...             ...    ...\n",
       "12786         saint      1\n",
       "12787    interlaken      1\n",
       "12788         askes      1\n",
       "12789        strain      1\n",
       "12790        solids      1\n",
       "\n",
       "[12791 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs1 = caseFolding_df.keys()\n",
    "cs2 = caseFolding_df.values()\n",
    "\n",
    "caseFolding_df = pd.DataFrame({\"Token\" : cs1, \"TF\" : cs2})\n",
    "caseFolding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bd2cf26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 50,\n",
       " 'comparison': 37,\n",
       " 'explanations': 45,\n",
       " 'given': 45,\n",
       " 'by': 50,\n",
       " 'explainable': 48,\n",
       " 'artificial': 48,\n",
       " 'intelligence': 48,\n",
       " 'methods': 47,\n",
       " 'on': 50,\n",
       " 'analysing': 4,\n",
       " 'electronic': 16,\n",
       " 'health': 29,\n",
       " 'records': 12,\n",
       " 'fan': 16,\n",
       " 'bruce': 0,\n",
       " 'burnett': 0,\n",
       " 'shang': 1,\n",
       " 'ming': 47,\n",
       " 'zhou': 3,\n",
       " 'swansea': 1,\n",
       " 'university': 5,\n",
       " 'united': 1,\n",
       " 'kingdom': 0,\n",
       " 'ac': 50,\n",
       " 'uk': 35,\n",
       " 'plymouth': 1,\n",
       " 'abstract': 21,\n",
       " 'aims': 35,\n",
       " 'provide': 48,\n",
       " 'intelligible': 13,\n",
       " 'users': 38,\n",
       " 'algorithms': 39,\n",
       " 'such': 50,\n",
       " 'as': 50,\n",
       " 'lime': 13,\n",
       " 'rules': 35,\n",
       " 'compute': 45,\n",
       " 'feature': 48,\n",
       " 'importance': 41,\n",
       " 'machine': 48,\n",
       " 'learning': 49,\n",
       " 'predictions': 41,\n",
       " 'has': 50,\n",
       " 'attracted': 6,\n",
       " 'much': 36,\n",
       " 'research': 50,\n",
       " 'attention': 36,\n",
       " 'applying': 22,\n",
       " 'techniques': 46,\n",
       " 'in': 50,\n",
       " 'healthcare': 16,\n",
       " 'inform': 48,\n",
       " 'clinical': 13,\n",
       " 'decision': 50,\n",
       " 'making': 43,\n",
       " 'is': 50,\n",
       " 'challenging': 20,\n",
       " 'paper': 40,\n",
       " 'tertiary': 3,\n",
       " 'extension': 16,\n",
       " 'complex': 45,\n",
       " 'large': 44,\n",
       " 'scale': 36,\n",
       " 'compare': 38,\n",
       " 'features': 48,\n",
       " 'terms': 44,\n",
       " 'prediction': 44,\n",
       " 'es': 50,\n",
       " 'models': 48,\n",
       " 'experimental': 25,\n",
       " 'results': 50,\n",
       " 'show': 47,\n",
       " 'studied': 17,\n",
       " 'circumstantially': 1,\n",
       " 'generate': 43,\n",
       " 'different': 50,\n",
       " 'top': 46,\n",
       " 'aberrations': 2,\n",
       " 'shared': 10,\n",
       " 'merit': 4,\n",
       " 'further': 44,\n",
       " 'exploration': 13,\n",
       " 'domain': 41,\n",
       " 'experts': 29,\n",
       " 'evaluate': 43,\n",
       " 'human': 46,\n",
       " 'trust': 41,\n",
       " 'index': 21,\n",
       " 'ai': 50,\n",
       " 'black': 44,\n",
       " 'box': 46,\n",
       " 'glass': 5,\n",
       " 'i': 50,\n",
       " 'introduction': 14,\n",
       " 'though': 44,\n",
       " 'machines': 26,\n",
       " 'outperform': 19,\n",
       " 'some': 45,\n",
       " 'cations': 48,\n",
       " 'one': 49,\n",
       " 'question': 35,\n",
       " 'remains': 15,\n",
       " 'can': 50,\n",
       " 'assure': 7,\n",
       " 'solutions': 26,\n",
       " 'are': 50,\n",
       " 'trustworthy': 15,\n",
       " 'commonalities': 2,\n",
       " 'arise': 19,\n",
       " 'algorithm': 44,\n",
       " 'applications': 40,\n",
       " 'model': 49,\n",
       " 'exhibit': 8,\n",
       " 'behaviours': 5,\n",
       " 'compromising': 4,\n",
       " 'factor': 39,\n",
       " 'become': 39,\n",
       " 'an': 50,\n",
       " 'adversary': 3,\n",
       " 'understanding': 40,\n",
       " 'internal': 24,\n",
       " 'mechanisms': 28,\n",
       " 'difficult': 34,\n",
       " 'not': 50,\n",
       " 'impossible': 8,\n",
       " 'medical': 34,\n",
       " 'sciences': 21,\n",
       " 'have': 50,\n",
       " 'witnessed': 2,\n",
       " 'growing': 15,\n",
       " 'interest': 46,\n",
       " 'using': 49,\n",
       " 'ml': 32,\n",
       " 'however': 26,\n",
       " 'light': 45,\n",
       " 'ensuring': 10,\n",
       " 'collaboration': 7,\n",
       " 'al': 50,\n",
       " 'looked': 10,\n",
       " 'at': 50,\n",
       " 'clinicians': 7,\n",
       " 'want': 25,\n",
       " 'identify': 40,\n",
       " 'merely': 6,\n",
       " 'having': 25,\n",
       " 'highly': 31,\n",
       " 'accurate': 38,\n",
       " 'sufficient': 34,\n",
       " 'notably': 3,\n",
       " 'single': 41,\n",
       " 'metric': 41,\n",
       " 'classification': 45,\n",
       " 'accuracy': 42,\n",
       " 'does': 42,\n",
       " 'insight': 37,\n",
       " 'solution': 36,\n",
       " 'was': 47,\n",
       " 'obtained': 26,\n",
       " 'or': 50,\n",
       " 'depth': 20,\n",
       " 'effectiveness': 32,\n",
       " 'medicine': 21,\n",
       " 'needs': 34,\n",
       " 'requisition': 1,\n",
       " 'clarity': 7,\n",
       " 'due': 43,\n",
       " 'fragile': 4,\n",
       " 'nature': 29,\n",
       " 'data': 50,\n",
       " 'field': 38,\n",
       " 'sub': 49,\n",
       " 'end': 50,\n",
       " 'user': 41,\n",
       " 'there': 48,\n",
       " 'pressing': 8,\n",
       " 'need': 47,\n",
       " 'develop': 47,\n",
       " 'tools': 34,\n",
       " 'traditional': 22,\n",
       " 'approaches': 45,\n",
       " 'suffer': 10,\n",
       " 'doubts': 1,\n",
       " 'work': 50,\n",
       " 'supported': 30,\n",
       " 'centre': 5,\n",
       " 'doctoral': 3,\n",
       " 'training': 46,\n",
       " 'advanced': 17,\n",
       " 'computing': 28,\n",
       " 'funded': 11,\n",
       " 'grant': 19,\n",
       " 'funding': 14,\n",
       " 'contribution': 31,\n",
       " 'welsh': 0,\n",
       " 'government': 10,\n",
       " 'office': 4,\n",
       " 'science': 35,\n",
       " 'cymru': 0,\n",
       " 'ii': 38,\n",
       " 'programme': 14,\n",
       " 'tackling': 4,\n",
       " 'general': 46,\n",
       " 'public': 36,\n",
       " 'adheres': 1,\n",
       " 'ethical': 20,\n",
       " 'concern': 30,\n",
       " 'considerations': 10,\n",
       " 'be': 50,\n",
       " 'made': 42,\n",
       " 'within': 39,\n",
       " 'bias': 36,\n",
       " 'discriminatory': 4,\n",
       " 'susceptible': 8,\n",
       " 'been': 48,\n",
       " 'thus': 35,\n",
       " 'developed': 40,\n",
       " 'applied': 42,\n",
       " 'it': 50,\n",
       " 'believed': 2,\n",
       " 'will': 48,\n",
       " 'needed': 28,\n",
       " 'critical': 30,\n",
       " 'apply': 36,\n",
       " 'three': 43,\n",
       " 'demonstrate': 31,\n",
       " 'usability': 15,\n",
       " 'appendix': 1,\n",
       " 'particular': 42,\n",
       " 'study': 46,\n",
       " 'lung': 7,\n",
       " 'cancer': 13,\n",
       " 'mortality': 4,\n",
       " 'multiple': 37,\n",
       " 'leading': 26,\n",
       " 'cause': 47,\n",
       " 'men': 50,\n",
       " 'women': 4,\n",
       " 'worldwide': 6,\n",
       " 'early': 29,\n",
       " 'predicting': 28,\n",
       " 'patients': 20,\n",
       " 'help': 41,\n",
       " 'benefit': 29,\n",
       " 'treatment': 16,\n",
       " 'risk': 31,\n",
       " 'relapse': 1,\n",
       " 'so': 50,\n",
       " 'professionals': 11,\n",
       " 'preventative': 1,\n",
       " 'measures': 32,\n",
       " 'plans': 6,\n",
       " 'used': 49,\n",
       " 'simulacrum': 0,\n",
       " 'synthetic': 8,\n",
       " 'created': 26,\n",
       " 'derived': 23,\n",
       " 'anonymous': 5,\n",
       " 'provided': 36,\n",
       " 'national': 45,\n",
       " 'registration': 1,\n",
       " 'analysis': 46,\n",
       " 'service': 22,\n",
       " 'part': 49,\n",
       " 'england': 0,\n",
       " 'experimentation': 3,\n",
       " 'focused': 31,\n",
       " 'generating': 28,\n",
       " 'regarding': 28,\n",
       " 'indicative': 2,\n",
       " 'therefore': 31,\n",
       " 'use': 50,\n",
       " 'allows': 29,\n",
       " 'us': 50,\n",
       " 'trends': 12,\n",
       " 'identified': 30,\n",
       " 'through': 48,\n",
       " 'expert': 39,\n",
       " 'deduce': 5,\n",
       " 'quantity': 5,\n",
       " 'compares': 6,\n",
       " 'state': 44,\n",
       " 'art': 50,\n",
       " 'serves': 12,\n",
       " 'demonstration': 7,\n",
       " 'may': 44,\n",
       " 'deliver': 19,\n",
       " 'il': 50,\n",
       " 'background': 21,\n",
       " 'focus': 43,\n",
       " 'experimenting': 2,\n",
       " 'attribution': 18,\n",
       " 'assign': 19,\n",
       " 'weights': 28,\n",
       " 'predict': 47,\n",
       " 'briefly': 13,\n",
       " 'review': 42,\n",
       " 'below': 25,\n",
       " 'shapley': 2,\n",
       " 'additive': 15,\n",
       " 'explains': 22,\n",
       " 'based': 50,\n",
       " 'values': 42,\n",
       " 'directs': 1,\n",
       " 'fairly': 8,\n",
       " 'distribute': 26,\n",
       " 'determined': 29,\n",
       " 'investments': 1,\n",
       " 'implementation': 27,\n",
       " 'value': 47,\n",
       " 'sees': 7,\n",
       " 'incorporation': 3,\n",
       " 'characteristic': 32,\n",
       " 'function': 42,\n",
       " 'number': 44,\n",
       " 'coalitions': 3,\n",
       " 'calculates': 8,\n",
       " 'average': 31,\n",
       " 'over': 50,\n",
       " 'permutation': 9,\n",
       " 'defined': 40,\n",
       " 'linear': 37,\n",
       " 'binary': 19,\n",
       " 'inputs': 31,\n",
       " 'explanation': 47,\n",
       " 'original': 34,\n",
       " 'coalition': 6,\n",
       " 'vector': 33,\n",
       " 'simplified': 17,\n",
       " 'maximum': 21,\n",
       " 'explanatory': 27,\n",
       " 'complexity': 34,\n",
       " 'input': 47,\n",
       " 'finally': 8,\n",
       " 'sum': 49,\n",
       " 'all': 50,\n",
       " 'contributions': 23,\n",
       " 'fit': 42,\n",
       " 'local': 45,\n",
       " 'interpretable': 42,\n",
       " 'agnostic': 36,\n",
       " 'close': 31,\n",
       " 'focuses': 19,\n",
       " 'achieved': 27,\n",
       " 'accessing': 2,\n",
       " 'fits': 24,\n",
       " 'line': 49,\n",
       " 'linearity': 6,\n",
       " 'regularization': 13,\n",
       " 'constraint': 24,\n",
       " 'regression': 27,\n",
       " 'obtain': 36,\n",
       " 'point': 44,\n",
       " 'faithfulness': 4,\n",
       " 'measured': 21,\n",
       " 'parameters': 36,\n",
       " 'denotes': 13,\n",
       " 'square': 17,\n",
       " 'loss': 27,\n",
       " 'minimized': 7,\n",
       " 'min': 50,\n",
       " 'lf': 48,\n",
       " 'anchors': 2,\n",
       " 'individual': 41,\n",
       " 'dictions': 41,\n",
       " 'prioritising': 2,\n",
       " 'search': 50,\n",
       " 'rule': 41,\n",
       " 'increase': 42,\n",
       " 'likelihood': 11,\n",
       " 'perturbation': 18,\n",
       " 'strategy': 24,\n",
       " 'create': 41,\n",
       " 'output': 46,\n",
       " 'low': 49,\n",
       " 'idea': 33,\n",
       " 'factual': 20,\n",
       " 'identifying': 32,\n",
       " 'cases': 42,\n",
       " 'certain': 44,\n",
       " 'give': 47,\n",
       " 'conditional': 12,\n",
       " 'statements': 14,\n",
       " 'true': 27,\n",
       " 'provides': 39,\n",
       " 'format': 48,\n",
       " 'then': 43,\n",
       " 'structure': 42,\n",
       " 'better': 40,\n",
       " 'localized': 4,\n",
       " 'anchor': 3,\n",
       " 'predicate': 4,\n",
       " 'expected': 28,\n",
       " 'evaluation': 40,\n",
       " 'neighbours': 4,\n",
       " 'instance': 43,\n",
       " 'distribution': 38,\n",
       " 'matching': 8,\n",
       " 'greater': 21,\n",
       " 'equal': 26,\n",
       " 'precision': 22,\n",
       " 'boundary': 14,\n",
       " 'set': 50,\n",
       " 'threshold': 13,\n",
       " 'iii': 14,\n",
       " 'preparation': 9,\n",
       " 'explaining': 44,\n",
       " 'specifically': 21,\n",
       " 'process': 50,\n",
       " 'solve': 27,\n",
       " 'following': 43,\n",
       " 'problem': 43,\n",
       " 'patient': 25,\n",
       " 'collected': 20,\n",
       " 'see': 46,\n",
       " 'table': 49,\n",
       " 'example': 50,\n",
       " 'he': 50,\n",
       " 'likely': 29,\n",
       " 'survive': 3,\n",
       " 'sample': 33,\n",
       " 'record': 21,\n",
       " 'after': 38,\n",
       " 'processing': 38,\n",
       " 'classifier': 40,\n",
       " 'consists': 24,\n",
       " 'allowing': 17,\n",
       " 'development': 38,\n",
       " 'maintaining': 11,\n",
       " 'confidentiality': 3,\n",
       " 'reflecting': 7,\n",
       " 'high': 49,\n",
       " 'degree': 33,\n",
       " 'properties': 31,\n",
       " 'found': 37,\n",
       " 'first': 50,\n",
       " 'performed': 32,\n",
       " 'construct': 30,\n",
       " 'clean': 10,\n",
       " 'support': 48,\n",
       " 'remove': 28,\n",
       " 'null': 7,\n",
       " 'obvious': 10,\n",
       " 'errors': 20,\n",
       " 'logical': 40,\n",
       " 'inconsistencies': 4,\n",
       " 'weight': 39,\n",
       " 'height': 7,\n",
       " 'unrealistic': 4,\n",
       " 'listed': 17,\n",
       " 'undergoing': 1,\n",
       " 'regimen': 1,\n",
       " 'death': 5,\n",
       " 'removed': 15,\n",
       " 'conditions': 30,\n",
       " 'amended': 1,\n",
       " 'assumption': 25,\n",
       " 'balanced': 16,\n",
       " 'lower': 33,\n",
       " 'bounds': 4,\n",
       " 'developing': 23,\n",
       " 'compose': 19,\n",
       " 'tabular': 7,\n",
       " 'each': 50,\n",
       " 'row': 42,\n",
       " 'corresponding': 24,\n",
       " 'treat': 30,\n",
       " 'category': 22,\n",
       " 'two': 50,\n",
       " 'classes': 23,\n",
       " 'alive': 3,\n",
       " 'deceased': 2,\n",
       " 'survived': 2,\n",
       " 'respectively': 36,\n",
       " 'assume': 31,\n",
       " 'knowledge': 49,\n",
       " 'association': 19,\n",
       " 'specificities': 1,\n",
       " 'reflective': 1,\n",
       " 'before': 28,\n",
       " 'running': 15,\n",
       " 'execute': 12,\n",
       " 'classifiers': 25,\n",
       " 'deterministic': 2,\n",
       " 'inheriting': 1,\n",
       " 'logistic': 11,\n",
       " 'also': 50,\n",
       " 'performance': 47,\n",
       " 'baseline': 19,\n",
       " 'method': 47,\n",
       " 'boosting': 11,\n",
       " 'displayed': 10,\n",
       " 'subset': 28,\n",
       " 'contains': 25,\n",
       " 'cohort': 6,\n",
       " 'selected': 33,\n",
       " 'trained': 37,\n",
       " 'tested': 24,\n",
       " 'remaining': 18,\n",
       " 'lr': 33,\n",
       " 'recall': 13,\n",
       " 'evident': 7,\n",
       " 'best': 43,\n",
       " 'performing': 26,\n",
       " 'drop': 18,\n",
       " 'both': 48,\n",
       " 'http': 45,\n",
       " 'www': 38,\n",
       " 'about': 43,\n",
       " 'iv': 50,\n",
       " 'classifications': 14,\n",
       " 'illustration': 7,\n",
       " 'present': 50,\n",
       " 'global': 35,\n",
       " 'testing': 29,\n",
       " 'figure': 20,\n",
       " 'observe': 35,\n",
       " 'presence': 18,\n",
       " 'absence': 7,\n",
       " 'distant': 0,\n",
       " 'metastatic': 0,\n",
       " 'spread': 13,\n",
       " 'size': 42,\n",
       " 'extent': 20,\n",
       " 'primary': 20,\n",
       " 'tumor': 10,\n",
       " 'involvement': 5,\n",
       " 'regional': 4,\n",
       " 'lymph': 2,\n",
       " 'nodes': 18,\n",
       " 'most': 50,\n",
       " 'similar': 44,\n",
       " 'figures': 10,\n",
       " 'produced': 24,\n",
       " 'omitted': 5,\n",
       " 'space': 34,\n",
       " 'limit': 41,\n",
       " 'age': 50,\n",
       " 'morph': 12,\n",
       " 'site': 31,\n",
       " 'sex': 5,\n",
       " 'grade': 9,\n",
       " 'plan': 49,\n",
       " 'ace': 48,\n",
       " 'trial': 23,\n",
       " 'ns': 50,\n",
       " 'time': 46,\n",
       " 'delay': 5,\n",
       " 'behaviour': 16,\n",
       " 'stopped': 1,\n",
       " 'outcome': 29,\n",
       " 'cycle': 13,\n",
       " 'radiation': 1,\n",
       " 'impact': 40,\n",
       " 'axis': 10,\n",
       " 'weighting': 4,\n",
       " 'red': 50,\n",
       " 'being': 39,\n",
       " 'shift': 18,\n",
       " 'countered': 6,\n",
       " 'blue': 13,\n",
       " 'minimal': 11,\n",
       " 'conduct': 26,\n",
       " 'quantitative': 15,\n",
       " 'between': 49,\n",
       " 'visualisation': 4,\n",
       " 'important': 46,\n",
       " 'means': 35,\n",
       " 'other': 50,\n",
       " 'parts': 26,\n",
       " 'body': 15,\n",
       " 'no': 50,\n",
       " 'influential': 13,\n",
       " 'ranking': 20,\n",
       " 'differs': 9,\n",
       " 'consider': 48,\n",
       " 'second': 43,\n",
       " 'considers': 19,\n",
       " 'higher': 37,\n",
       " 'ft': 49,\n",
       " 'ed': 50,\n",
       " 'width': 15,\n",
       " 'descriptive': 11,\n",
       " 'block': 18,\n",
       " 'colour': 5,\n",
       " 'probability': 24,\n",
       " 'noticing': 0,\n",
       " 'discrepancy': 1,\n",
       " 'differences': 23,\n",
       " 'absolute': 13,\n",
       " 'irrespective': 3,\n",
       " 'count': 45,\n",
       " 'differ': 50,\n",
       " 'probabilities': 14,\n",
       " 'dead': 1,\n",
       " 'maul': 0,\n",
       " 'dose': 2,\n",
       " 'cur': 50,\n",
       " 'providing': 36,\n",
       " 'class': 47,\n",
       " 'coverage': 6,\n",
       " 'junction': 11,\n",
       " 'case': 48,\n",
       " 'words': 39,\n",
       " 'share': 22,\n",
       " 'written': 18,\n",
       " 'shown': 38,\n",
       " 'seen': 27,\n",
       " 'exist': 44,\n",
       " 'relationship': 30,\n",
       " 'indexed': 4,\n",
       " 'same': 43,\n",
       " 'across': 29,\n",
       " 'but': 50,\n",
       " 'differential': 6,\n",
       " 'comparisons': 10,\n",
       " 'majority': 19,\n",
       " 'priority': 5,\n",
       " 'note': 40,\n",
       " 'included': 20,\n",
       " 'always': 30,\n",
       " 'find': 45,\n",
       " 'quality': 33,\n",
       " 'response': 32,\n",
       " 'collection': 23,\n",
       " 'instances': 22,\n",
       " 'applicable': 23,\n",
       " 'kindred': 1,\n",
       " 'calculation': 13,\n",
       " 'hold': 33,\n",
       " 'little': 15,\n",
       " 'integrity': 3,\n",
       " 'conversely': 1,\n",
       " 'more': 50,\n",
       " 'meaningful': 19,\n",
       " 'comparing': 20,\n",
       " 'information': 46,\n",
       " 'optimal': 17,\n",
       " 'relational': 8,\n",
       " 'form': 50,\n",
       " 'validation': 25,\n",
       " 'supporting': 18,\n",
       " 'consistency': 15,\n",
       " 'representation': 38,\n",
       " 'extracted': 25,\n",
       " 'taken': 30,\n",
       " 'test': 46,\n",
       " 'extract': 35,\n",
       " 'under': 50,\n",
       " 'condition': 36,\n",
       " 'well': 47,\n",
       " 'way': 46,\n",
       " 'factors': 29,\n",
       " 'were': 44,\n",
       " 'either': 36,\n",
       " 'returned': 8,\n",
       " 'particularly': 17,\n",
       " 'interesting': 19,\n",
       " 'result': 50,\n",
       " 'confirms': 9,\n",
       " 'superiority': 3,\n",
       " 'stage': 25,\n",
       " 'staging': 1,\n",
       " 'step': 38,\n",
       " 'diagnosis': 26,\n",
       " 'multifarious': 1,\n",
       " 'objectives': 14,\n",
       " 'helping': 9,\n",
       " 'indication': 6,\n",
       " 'prognosis': 6,\n",
       " 'showing': 20,\n",
       " 'facilitating': 8,\n",
       " 'exchange': 7,\n",
       " 'de': 50,\n",
       " 'findings': 25,\n",
       " 'consistent': 25,\n",
       " 'another': 39,\n",
       " 'indicating': 20,\n",
       " 'prognostic': 3,\n",
       " 'while': 43,\n",
       " 'followed': 22,\n",
       " 'histologic': 2,\n",
       " 'status': 17,\n",
       " 'shows': 36,\n",
       " 'morphology': 3,\n",
       " 'conclusion': 27,\n",
       " 'considered': 40,\n",
       " 'answer': 29,\n",
       " 'compared': 32,\n",
       " 'answering': 12,\n",
       " 'illustrations': 1,\n",
       " 'bring': 17,\n",
       " 'communicating': 3,\n",
       " 'just': 44,\n",
       " 'reasoning': 35,\n",
       " 'believe': 19,\n",
       " 'improve': 44,\n",
       " 'rate': 50,\n",
       " 'deduction': 3,\n",
       " 'deciding': 8,\n",
       " 'coinciding': 1,\n",
       " 'known': 36,\n",
       " 'edge': 50,\n",
       " 'secondary': 12,\n",
       " 'clear': 34,\n",
       " 'supports': 15,\n",
       " 'substitute': 3,\n",
       " 'discernible': 2,\n",
       " 'necessity': 9,\n",
       " 'conducted': 16,\n",
       " 'future': 45,\n",
       " 'determine': 40,\n",
       " 'references': 14,\n",
       " 'miller': 4,\n",
       " 'insights': 30,\n",
       " 'social': 30,\n",
       " 'volume': 18,\n",
       " 'february': 0,\n",
       " 'pages': 11,\n",
       " 'gold': 3,\n",
       " 'explain': 50,\n",
       " 'able': 50,\n",
       " 'rigorous': 11,\n",
       " 'lent': 27,\n",
       " 'system': 49,\n",
       " 'small': 37,\n",
       " 'unit': 44,\n",
       " 'tactical': 4,\n",
       " 'behavior': 33,\n",
       " 'peeking': 0,\n",
       " 'inside': 23,\n",
       " 'survey': 39,\n",
       " 'open': 40,\n",
       " 'access': 35,\n",
       " 'journal': 27,\n",
       " 'watson': 1,\n",
       " 'heart': 14,\n",
       " 'attack': 17,\n",
       " 'computational': 27,\n",
       " 'iccs': 0,\n",
       " 'th': 50,\n",
       " 'international': 23,\n",
       " 'conference': 25,\n",
       " 'springer': 6,\n",
       " 'lange': 0,\n",
       " 'acute': 6,\n",
       " 'illness': 3,\n",
       " 'gandhi': 0,\n",
       " 'powell': 0,\n",
       " 'epidemiology': 2,\n",
       " 'glob': 37,\n",
       " 'lee': 10,\n",
       " 'unified': 20,\n",
       " 'approach': 50,\n",
       " 'interpreting': 23,\n",
       " 'advances': 15,\n",
       " 'annual': 12,\n",
       " 'why': 36,\n",
       " 'any': 50,\n",
       " 'pro': 50,\n",
       " 'thirty': 0,\n",
       " 'ar': 50,\n",
       " 'press': 38,\n",
       " 'koch': 0,\n",
       " 'inter': 50,\n",
       " 'framework': 41,\n",
       " 'yang': 7,\n",
       " 'biology': 5,\n",
       " 'options': 8,\n",
       " 'jones': 1,\n",
       " 'publishing': 1,\n",
       " 'taylor': 0,\n",
       " 'group': 34,\n",
       " 'geographical': 4,\n",
       " 'rays': 5,\n",
       " 'print': 23,\n",
       " 'online': 36,\n",
       " 'homepage': 3,\n",
       " 'https': 44,\n",
       " 'task': 40,\n",
       " 'specific': 47,\n",
       " 'temporal': 14,\n",
       " 'units': 21,\n",
       " 'li': 50,\n",
       " 'yi': 50,\n",
       " 'wu': 3,\n",
       " 'cite': 22,\n",
       " 'article': 33,\n",
       " 'doi': 36,\n",
       " 'link': 23,\n",
       " 'published': 27,\n",
       " 'aug': 26,\n",
       " 'sg': 14,\n",
       " 'submit': 8,\n",
       " 'views': 23,\n",
       " 'view': 50,\n",
       " 'related': 43,\n",
       " 'articles': 16,\n",
       " 'citing': 3,\n",
       " 'full': 42,\n",
       " 'action': 48,\n",
       " 'remote': 6,\n",
       " 'sensing': 5,\n",
       " 'systems': 44,\n",
       " 'school': 7,\n",
       " 'earth': 3,\n",
       " 'peking': 0,\n",
       " 'beijing': 0,\n",
       " 'china': 0,\n",
       " 'center': 21,\n",
       " 'digital': 15,\n",
       " 'spatial': 12,\n",
       " 'studies': 40,\n",
       " 'department': 2,\n",
       " 'geography': 3,\n",
       " 'geographic': 4,\n",
       " 'illinois': 0,\n",
       " 'urbana': 0,\n",
       " 'champaign': 0,\n",
       " 'usa': 41,\n",
       " 'info': 48,\n",
       " 'physics': 7,\n",
       " 'central': 14,\n",
       " 'south': 2,\n",
       " 'hunan': 0,\n",
       " 'history': 14,\n",
       " 'big': 28,\n",
       " 'often': 37,\n",
       " 'aggregated': 10,\n",
       " 'according': 34,\n",
       " 'received': 26,\n",
       " 'october': 0,\n",
       " 'analyzing': 20,\n",
       " 'activities': 18,\n",
       " 'urban': 7,\n",
       " 'environments': 11,\n",
       " 'many': 44,\n",
       " 'accepted': 18,\n",
       " 'july': 0,\n",
       " 'categorize': 19,\n",
       " 'groups': 25,\n",
       " 'characteristics': 27,\n",
       " 'vary': 25,\n",
       " 'deep': 44,\n",
       " 'tensor': 7,\n",
       " 'essential': 29,\n",
       " 'apparently': 6,\n",
       " 'dependence': 16,\n",
       " 'variety': 24,\n",
       " 'tasks': 34,\n",
       " 'impede': 5,\n",
       " 'effective': 45,\n",
       " 'assessment': 31,\n",
       " 'inspired': 14,\n",
       " 'image': 35,\n",
       " 'components': 30,\n",
       " 'propose': 41,\n",
       " 'layer': 39,\n",
       " 'wise': 30,\n",
       " 'relevance': 17,\n",
       " 'propagation': 12,\n",
       " 'assess': 43,\n",
       " 'organizes': 2,\n",
       " 'extensible': 2,\n",
       " 'dimensional': 24,\n",
       " 'labeling': 4,\n",
       " 'typical': 31,\n",
       " 'temporally': 2,\n",
       " 'spatially': 1,\n",
       " 'relevant': 36,\n",
       " 'neural': 43,\n",
       " 'network': 47,\n",
       " 'proceeds': 2,\n",
       " 'analytical': 12,\n",
       " 'propagates': 2,\n",
       " 'backward': 9,\n",
       " 'taxi': 1,\n",
       " 'trajectory': 4,\n",
       " 'validates': 1,\n",
       " 'prove': 47,\n",
       " 'proposed': 40,\n",
       " 'attempts': 12,\n",
       " 'discover': 33,\n",
       " 'phenomena': 16,\n",
       " 'change': 40,\n",
       " 'constantly': 2,\n",
       " 'variation': 21,\n",
       " 'widely': 28,\n",
       " 'represent': 48,\n",
       " 'activity': 15,\n",
       " 'patterns': 26,\n",
       " 'environmental': 5,\n",
       " 'natural': 31,\n",
       " 'ma': 50,\n",
       " 'preset': 1,\n",
       " 'resolutions': 3,\n",
       " 'tags': 5,\n",
       " 'map': 35,\n",
       " 'integrate': 26,\n",
       " 'tessellations': 1,\n",
       " 'composed': 15,\n",
       " 'series': 21,\n",
       " 'pei': 5,\n",
       " 'contact': 5,\n",
       " 'pku': 1,\n",
       " 'limited': 35,\n",
       " 'trading': 7,\n",
       " 'panel': 8,\n",
       " 'definition': 29,\n",
       " 'section': 36,\n",
       " 'acquire': 16,\n",
       " 'extracting': 19,\n",
       " 'daily': 9,\n",
       " 'trip': 4,\n",
       " 'weather': 5,\n",
       " 'heterogeneity': 5,\n",
       " 'pay': 17,\n",
       " 'poorly': 5,\n",
       " 'drained': 1,\n",
       " 'zones': 4,\n",
       " 'previously': 22,\n",
       " 'mentioned': 31,\n",
       " 'only': 47,\n",
       " 'beneficial': 10,\n",
       " 'guiding': 8,\n",
       " 'discovering': 5,\n",
       " 'few': 37,\n",
       " 'existing': 36,\n",
       " 'topic': 27,\n",
       " 'easy': 24,\n",
       " 'gao': 2,\n",
       " 'assessments': 9,\n",
       " 'independent': 28,\n",
       " 'variables': 30,\n",
       " 'sensitivity': 15,\n",
       " 'changes': 28,\n",
       " 'depending': 17,\n",
       " 'even': 46,\n",
       " 'necessary': 32,\n",
       " 'regard': 41,\n",
       " 'daytime': 1,\n",
       " 'valuable': 21,\n",
       " 'nighttime': 1,\n",
       " 'traffic': 13,\n",
       " 'management': 16,\n",
       " 'people': 27,\n",
       " 'active': 40,\n",
       " 'crucial': 25,\n",
       " 'criminal': 11,\n",
       " 'classic': 11,\n",
       " 'analyses': 15,\n",
       " 'density': 12,\n",
       " 'clustering': 20,\n",
       " 'procedure': 26,\n",
       " 'specialized': 4,\n",
       " 'extraction': 18,\n",
       " 'correspond': 34,\n",
       " 'desirable': 17,\n",
       " 'various': 34,\n",
       " 'its': 50,\n",
       " 'excellent': 9,\n",
       " 'applicability': 9,\n",
       " 'reichstein': 0,\n",
       " 'meanwhile': 0,\n",
       " 'computer': 32,\n",
       " 'vision': 38,\n",
       " 'proved': 32,\n",
       " 'networks': 44,\n",
       " 'images': 27,\n",
       " 'address': 36,\n",
       " 'difficulties': 8,\n",
       " 'earlier': 17,\n",
       " 'recurrent': 13,\n",
       " 'capture': 32,\n",
       " 'contrast': 30,\n",
       " 'acquired': 10,\n",
       " 'implicit': 20,\n",
       " 'interpret': 49,\n",
       " 'researchers': 30,\n",
       " 'several': 38,\n",
       " 'bach': 15,\n",
       " 'activation': 24,\n",
       " 'mapping': 22,\n",
       " 'cam': 26,\n",
       " 'reference': 25,\n",
       " 'label': 31,\n",
       " 'propagate': 8,\n",
       " 'back': 43,\n",
       " 'concepts': 28,\n",
       " 'basic': 25,\n",
       " 'principles': 15,\n",
       " 'objective': 29,\n",
       " 'distinguish': 19,\n",
       " 'pick': 12,\n",
       " 'up': 50,\n",
       " 'weekdays': 1,\n",
       " 'weekends': 1,\n",
       " 'holidays': 1,\n",
       " 'gives': 22,\n",
       " 'spatiotemporal': 2,\n",
       " ...}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caseFolding_dff = df(caseFolding_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15aeb71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "caseFolding_df['df'] = caseFolding_dff.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72767b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>TF</th>\n",
       "      <th>df</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>10140</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comparison</td>\n",
       "      <td>129</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>explanations</td>\n",
       "      <td>1730</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>given</td>\n",
       "      <td>340</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>by</td>\n",
       "      <td>2360</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12786</th>\n",
       "      <td>saint</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12787</th>\n",
       "      <td>interlaken</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12788</th>\n",
       "      <td>askes</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12789</th>\n",
       "      <td>strain</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12790</th>\n",
       "      <td>solids</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12791 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Token     TF  df\n",
       "0                 a  10140  50\n",
       "1        comparison    129  37\n",
       "2      explanations   1730  45\n",
       "3             given    340  45\n",
       "4                by   2360  50\n",
       "...             ...    ...  ..\n",
       "12786         saint      1   1\n",
       "12787    interlaken      1   0\n",
       "12788         askes      1   0\n",
       "12789        strain      1  28\n",
       "12790        solids      1   0\n",
       "\n",
       "[12791 rows x 3 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caseFolding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "575332f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "caseFolding_df['idf'] = np.log((50 / caseFolding_df['df']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e6a4048",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>TF</th>\n",
       "      <th>df</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>10140</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comparison</td>\n",
       "      <td>129</td>\n",
       "      <td>37</td>\n",
       "      <td>0.301105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>explanations</td>\n",
       "      <td>1730</td>\n",
       "      <td>45</td>\n",
       "      <td>0.105361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>given</td>\n",
       "      <td>340</td>\n",
       "      <td>45</td>\n",
       "      <td>0.105361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>by</td>\n",
       "      <td>2360</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12786</th>\n",
       "      <td>saint</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.912023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12787</th>\n",
       "      <td>interlaken</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12788</th>\n",
       "      <td>askes</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12789</th>\n",
       "      <td>strain</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.579818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12790</th>\n",
       "      <td>solids</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12791 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Token     TF  df       idf\n",
       "0                 a  10140  50  0.000000\n",
       "1        comparison    129  37  0.301105\n",
       "2      explanations   1730  45  0.105361\n",
       "3             given    340  45  0.105361\n",
       "4                by   2360  50  0.000000\n",
       "...             ...    ...  ..       ...\n",
       "12786         saint      1   1  3.912023\n",
       "12787    interlaken      1   0       inf\n",
       "12788         askes      1   0       inf\n",
       "12789        strain      1  28  0.579818\n",
       "12790        solids      1   0       inf\n",
       "\n",
       "[12791 rows x 4 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "758d94a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>TF</th>\n",
       "      <th>df</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>10140</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>in</td>\n",
       "      <td>9282</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>is</td>\n",
       "      <td>4941</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>on</td>\n",
       "      <td>3711</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>as</td>\n",
       "      <td>3154</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>are</td>\n",
       "      <td>2873</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>model</td>\n",
       "      <td>2619</td>\n",
       "      <td>49</td>\n",
       "      <td>0.020203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>be</td>\n",
       "      <td>2409</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>by</td>\n",
       "      <td>2360</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>an</td>\n",
       "      <td>2261</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Token     TF  df       idf\n",
       "0        a  10140  50  0.000000\n",
       "50      in   9282  50  0.000000\n",
       "56      is   4941  50  0.000000\n",
       "9       on   3711  50  0.000000\n",
       "34      as   3154  50  0.000000\n",
       "106    are   2873  50  0.000000\n",
       "112  model   2619  49  0.020203\n",
       "208     be   2409  50  0.000000\n",
       "4       by   2360  50  0.000000\n",
       "118     an   2261  50  0.000000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caseFol_10 = caseFolding_df.sort_values(by=['TF'], ascending=False)[0:10]\n",
    "caseFol_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804c7170",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8376233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4a42e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316399"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_tok = [stemmer.stem(word) for word in our_tokens]\n",
    "len(stemmed_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43979077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 10140,\n",
       " 'comparison': 148,\n",
       " 'explan': 3531,\n",
       " 'given': 340,\n",
       " 'by': 2360,\n",
       " 'explain': 2147,\n",
       " 'artifici': 959,\n",
       " 'intellig': 1187,\n",
       " 'method': 1441,\n",
       " 'on': 3711,\n",
       " 'analys': 87,\n",
       " 'electron': 89,\n",
       " 'health': 101,\n",
       " 'record': 65,\n",
       " 'fan': 14,\n",
       " 'bruce': 1,\n",
       " 'burnett': 12,\n",
       " 'shang': 3,\n",
       " 'ming': 5,\n",
       " 'zhou': 41,\n",
       " 'swansea': 3,\n",
       " 'univers': 377,\n",
       " 'unit': 363,\n",
       " 'kingdom': 10,\n",
       " 'ac': 63,\n",
       " 'uk': 58,\n",
       " 'plymouth': 3,\n",
       " 'abstract': 159,\n",
       " 'aim': 170,\n",
       " 'provid': 853,\n",
       " 'user': 1165,\n",
       " 'algorithm': 919,\n",
       " 'such': 908,\n",
       " 'as': 3154,\n",
       " 'lime': 265,\n",
       " 'rule': 601,\n",
       " 'comput': 879,\n",
       " 'featur': 1748,\n",
       " 'import': 766,\n",
       " 'machin': 1283,\n",
       " 'learn': 2112,\n",
       " 'predict': 1308,\n",
       " 'ha': 890,\n",
       " 'attract': 15,\n",
       " 'much': 107,\n",
       " 'research': 929,\n",
       " 'attent': 216,\n",
       " 'appli': 477,\n",
       " 'techniqu': 556,\n",
       " 'in': 9283,\n",
       " 'healthcar': 92,\n",
       " 'inform': 974,\n",
       " 'clinic': 124,\n",
       " 'decis': 1437,\n",
       " 'make': 624,\n",
       " 'is': 4941,\n",
       " 'challeng': 337,\n",
       " 'paper': 300,\n",
       " 'tertiari': 6,\n",
       " 'extens': 65,\n",
       " 'complex': 321,\n",
       " 'larg': 169,\n",
       " 'scale': 126,\n",
       " 'compar': 315,\n",
       " 'term': 378,\n",
       " 'es': 41,\n",
       " 'model': 4217,\n",
       " 'experiment': 110,\n",
       " 'result': 782,\n",
       " 'show': 385,\n",
       " 'studi': 732,\n",
       " 'circumstanti': 1,\n",
       " 'gener': 1065,\n",
       " 'differ': 1013,\n",
       " 'top': 220,\n",
       " 'aberr': 4,\n",
       " 'share': 56,\n",
       " 'merit': 4,\n",
       " 'further': 175,\n",
       " 'explor': 116,\n",
       " 'domain': 510,\n",
       " 'expert': 297,\n",
       " 'evalu': 730,\n",
       " 'human': 1025,\n",
       " 'trust': 409,\n",
       " 'index': 70,\n",
       " 'ai': 1321,\n",
       " 'black': 462,\n",
       " 'box': 521,\n",
       " 'glass': 8,\n",
       " 'i': 902,\n",
       " 'introduct': 78,\n",
       " 'though': 43,\n",
       " 'outperform': 43,\n",
       " 'some': 460,\n",
       " 'cation': 23,\n",
       " 'one': 726,\n",
       " 'question': 306,\n",
       " 'remain': 84,\n",
       " 'can': 1794,\n",
       " 'assur': 18,\n",
       " 'solut': 200,\n",
       " 'are': 2874,\n",
       " 'trustworthi': 61,\n",
       " 'common': 152,\n",
       " 'aris': 30,\n",
       " 'applic': 665,\n",
       " 'exhibit': 10,\n",
       " 'behaviour': 66,\n",
       " 'compromis': 22,\n",
       " 'factor': 204,\n",
       " 'becom': 131,\n",
       " 'an': 2261,\n",
       " 'adversari': 7,\n",
       " 'understand': 817,\n",
       " 'intern': 551,\n",
       " 'mechan': 150,\n",
       " 'difficult': 80,\n",
       " 'not': 1144,\n",
       " 'imposs': 11,\n",
       " 'medic': 256,\n",
       " 'scienc': 592,\n",
       " 'have': 991,\n",
       " 'wit': 4,\n",
       " 'grow': 43,\n",
       " 'interest': 209,\n",
       " 'use': 2467,\n",
       " 'ml': 377,\n",
       " 'howev': 368,\n",
       " 'light': 35,\n",
       " 'ensur': 63,\n",
       " 'collabor': 47,\n",
       " 'al': 1377,\n",
       " 'look': 53,\n",
       " 'at': 807,\n",
       " 'clinician': 25,\n",
       " 'want': 118,\n",
       " 'identifi': 300,\n",
       " 'mere': 19,\n",
       " 'highli': 72,\n",
       " 'accur': 133,\n",
       " 'suffici': 60,\n",
       " 'notabl': 25,\n",
       " 'singl': 145,\n",
       " 'metric': 255,\n",
       " 'classif': 686,\n",
       " 'accuraci': 583,\n",
       " 'doe': 207,\n",
       " 'insight': 147,\n",
       " 'wa': 933,\n",
       " 'obtain': 200,\n",
       " 'or': 1571,\n",
       " 'depth': 52,\n",
       " 'effect': 434,\n",
       " 'medicin': 52,\n",
       " 'need': 361,\n",
       " 'requisit': 1,\n",
       " 'clariti': 13,\n",
       " 'due': 157,\n",
       " 'fragil': 11,\n",
       " 'natur': 322,\n",
       " 'data': 2120,\n",
       " 'field': 213,\n",
       " 'sub': 53,\n",
       " 'end': 154,\n",
       " 'there': 452,\n",
       " 'press': 115,\n",
       " 'develop': 584,\n",
       " 'tool': 188,\n",
       " 'tradit': 61,\n",
       " 'approach': 884,\n",
       " 'suffer': 16,\n",
       " 'doubt': 7,\n",
       " 'work': 669,\n",
       " 'support': 342,\n",
       " 'centr': 42,\n",
       " 'doctor': 80,\n",
       " 'train': 696,\n",
       " 'advanc': 193,\n",
       " 'fund': 67,\n",
       " 'grant': 44,\n",
       " 'contribut': 375,\n",
       " 'welsh': 1,\n",
       " 'govern': 56,\n",
       " 'offic': 15,\n",
       " 'cymru': 1,\n",
       " 'ii': 109,\n",
       " 'programm': 20,\n",
       " 'tackl': 21,\n",
       " 'public': 196,\n",
       " 'adher': 9,\n",
       " 'ethic': 123,\n",
       " 'concern': 121,\n",
       " 'consider': 61,\n",
       " 'be': 2617,\n",
       " 'made': 178,\n",
       " 'within': 192,\n",
       " 'bia': 117,\n",
       " 'discriminatori': 5,\n",
       " 'suscept': 12,\n",
       " 'been': 596,\n",
       " 'thu': 203,\n",
       " 'it': 2033,\n",
       " 'believ': 64,\n",
       " 'will': 468,\n",
       " 'critic': 130,\n",
       " 'three': 305,\n",
       " 'demonstr': 132,\n",
       " 'usabl': 19,\n",
       " 'appendix': 16,\n",
       " 'particular': 224,\n",
       " 'lung': 29,\n",
       " 'cancer': 63,\n",
       " 'mortal': 26,\n",
       " 'multipl': 139,\n",
       " 'lead': 154,\n",
       " 'caus': 157,\n",
       " 'men': 7,\n",
       " 'women': 12,\n",
       " 'worldwid': 14,\n",
       " 'earli': 71,\n",
       " 'patient': 210,\n",
       " 'help': 221,\n",
       " 'benefit': 68,\n",
       " 'treatment': 56,\n",
       " 'risk': 196,\n",
       " 'relaps': 1,\n",
       " 'so': 224,\n",
       " 'profession': 35,\n",
       " 'prevent': 45,\n",
       " 'measur': 596,\n",
       " 'plan': 57,\n",
       " 'simulacrum': 6,\n",
       " 'synthet': 24,\n",
       " 'creat': 252,\n",
       " 'deriv': 76,\n",
       " 'anonym': 6,\n",
       " 'nation': 119,\n",
       " 'registr': 2,\n",
       " 'analysi': 497,\n",
       " 'servic': 143,\n",
       " 'part': 188,\n",
       " 'england': 4,\n",
       " 'focus': 126,\n",
       " 'regard': 123,\n",
       " 'indic': 219,\n",
       " 'therefor': 202,\n",
       " 'allow': 187,\n",
       " 'us': 112,\n",
       " 'trend': 65,\n",
       " 'through': 299,\n",
       " 'deduc': 7,\n",
       " 'quantiti': 11,\n",
       " 'state': 298,\n",
       " 'art': 127,\n",
       " 'serv': 50,\n",
       " 'may': 615,\n",
       " 'deliv': 39,\n",
       " 'il': 34,\n",
       " 'background': 95,\n",
       " 'focu': 126,\n",
       " 'experi': 220,\n",
       " 'attribut': 260,\n",
       " 'assign': 102,\n",
       " 'weight': 176,\n",
       " 'briefli': 25,\n",
       " 'review': 337,\n",
       " 'below': 66,\n",
       " 'shapley': 73,\n",
       " 'addit': 317,\n",
       " 'base': 1643,\n",
       " 'valu': 809,\n",
       " 'direct': 177,\n",
       " 'fairli': 9,\n",
       " 'distribut': 245,\n",
       " 'determin': 204,\n",
       " 'invest': 10,\n",
       " 'implement': 191,\n",
       " 'see': 224,\n",
       " 'incorpor': 69,\n",
       " 'characterist': 112,\n",
       " 'function': 382,\n",
       " 'number': 472,\n",
       " 'coalit': 16,\n",
       " 'calcul': 125,\n",
       " 'averag': 129,\n",
       " 'over': 200,\n",
       " 'permut': 68,\n",
       " 'defin': 266,\n",
       " 'linear': 206,\n",
       " 'binari': 39,\n",
       " 'input': 616,\n",
       " 'origin': 184,\n",
       " 'vector': 194,\n",
       " 'simplifi': 33,\n",
       " 'maximum': 58,\n",
       " 'explanatori': 143,\n",
       " 'final': 214,\n",
       " 'sum': 70,\n",
       " 'all': 586,\n",
       " 'fit': 73,\n",
       " 'local': 566,\n",
       " 'interpret': 886,\n",
       " 'agnost': 176,\n",
       " 'close': 100,\n",
       " 'achiev': 240,\n",
       " 'access': 393,\n",
       " 'line': 124,\n",
       " 'regular': 39,\n",
       " 'constraint': 61,\n",
       " 'regress': 158,\n",
       " 'point': 464,\n",
       " 'faith': 34,\n",
       " 'paramet': 312,\n",
       " 'denot': 54,\n",
       " 'squar': 38,\n",
       " 'loss': 129,\n",
       " 'minim': 47,\n",
       " 'min': 77,\n",
       " 'lf': 13,\n",
       " 'anchor': 31,\n",
       " 'individu': 223,\n",
       " 'diction': 13,\n",
       " 'prioritis': 3,\n",
       " 'search': 110,\n",
       " 'increas': 233,\n",
       " 'likelihood': 23,\n",
       " 'perturb': 92,\n",
       " 'strategi': 121,\n",
       " 'output': 519,\n",
       " 'low': 180,\n",
       " 'idea': 96,\n",
       " 'factual': 14,\n",
       " 'case': 582,\n",
       " 'certain': 145,\n",
       " 'give': 158,\n",
       " 'condit': 181,\n",
       " 'statement': 119,\n",
       " 'true': 74,\n",
       " 'format': 67,\n",
       " 'then': 365,\n",
       " 'structur': 283,\n",
       " 'better': 178,\n",
       " 'predic': 18,\n",
       " 'expect': 117,\n",
       " 'neighbour': 15,\n",
       " 'instanc': 293,\n",
       " 'match': 48,\n",
       " 'greater': 36,\n",
       " 'equal': 49,\n",
       " 'precis': 131,\n",
       " 'boundari': 54,\n",
       " 'set': 787,\n",
       " 'threshold': 63,\n",
       " 'iii': 54,\n",
       " 'prepar': 44,\n",
       " 'specif': 508,\n",
       " 'process': 1118,\n",
       " 'solv': 77,\n",
       " 'follow': 394,\n",
       " 'problem': 390,\n",
       " 'collect': 158,\n",
       " 'tabl': 379,\n",
       " 'exampl': 671,\n",
       " 'he': 203,\n",
       " 'like': 247,\n",
       " 'surviv': 41,\n",
       " 'sampl': 326,\n",
       " 'after': 208,\n",
       " 'classifi': 601,\n",
       " 'consist': 190,\n",
       " 'maintain': 35,\n",
       " 'confidenti': 8,\n",
       " 'reflect': 130,\n",
       " 'high': 419,\n",
       " 'degre': 130,\n",
       " 'properti': 154,\n",
       " 'found': 140,\n",
       " 'first': 336,\n",
       " 'perform': 819,\n",
       " 'construct': 121,\n",
       " 'clean': 16,\n",
       " 'remov': 52,\n",
       " 'null': 10,\n",
       " 'obviou': 8,\n",
       " 'error': 130,\n",
       " 'logic': 207,\n",
       " 'inconsist': 12,\n",
       " 'height': 12,\n",
       " 'unrealist': 9,\n",
       " 'list': 149,\n",
       " 'undergo': 2,\n",
       " 'regimen': 1,\n",
       " 'death': 11,\n",
       " 'amend': 1,\n",
       " 'assumpt': 88,\n",
       " 'balanc': 65,\n",
       " 'lower': 87,\n",
       " 'bound': 17,\n",
       " 'compos': 31,\n",
       " 'tabular': 11,\n",
       " 'each': 610,\n",
       " 'row': 20,\n",
       " 'correspond': 167,\n",
       " 'treat': 38,\n",
       " 'categori': 200,\n",
       " 'two': 515,\n",
       " 'class': 391,\n",
       " 'aliv': 9,\n",
       " 'deceas': 6,\n",
       " 'respect': 211,\n",
       " 'assum': 93,\n",
       " 'knowledg': 722,\n",
       " 'associ': 341,\n",
       " 'befor': 69,\n",
       " 'run': 92,\n",
       " 'execut': 57,\n",
       " 'determinist': 3,\n",
       " 'inherit': 2,\n",
       " 'logist': 43,\n",
       " 'also': 715,\n",
       " 'baselin': 59,\n",
       " 'boost': 68,\n",
       " 'display': 48,\n",
       " 'subset': 95,\n",
       " 'contain': 173,\n",
       " 'cohort': 11,\n",
       " 'select': 457,\n",
       " 'test': 649,\n",
       " 'lr': 7,\n",
       " 'recal': 73,\n",
       " 'evid': 146,\n",
       " 'best': 191,\n",
       " 'drop': 35,\n",
       " 'both': 292,\n",
       " 'http': 832,\n",
       " 'www': 142,\n",
       " 'about': 317,\n",
       " 'iv': 52,\n",
       " 'illustr': 124,\n",
       " 'present': 436,\n",
       " 'global': 231,\n",
       " 'figur': 675,\n",
       " 'observ': 256,\n",
       " 'presenc': 52,\n",
       " 'absenc': 14,\n",
       " 'distant': 1,\n",
       " 'metastat': 1,\n",
       " 'spread': 11,\n",
       " 'size': 163,\n",
       " 'extent': 56,\n",
       " 'primari': 52,\n",
       " 'tumor': 25,\n",
       " 'involv': 118,\n",
       " 'region': 89,\n",
       " 'lymph': 3,\n",
       " 'node': 130,\n",
       " 'most': 406,\n",
       " 'similar': 234,\n",
       " 'produc': 227,\n",
       " 'omit': 14,\n",
       " 'space': 130,\n",
       " 'limit': 252,\n",
       " 'age': 90,\n",
       " 'morph': 6,\n",
       " 'site': 38,\n",
       " 'sex': 4,\n",
       " 'grade': 17,\n",
       " 'ace': 10,\n",
       " 'trial': 26,\n",
       " 'ns': 6,\n",
       " 'time': 482,\n",
       " 'delay': 8,\n",
       " 'stop': 47,\n",
       " 'outcom': 199,\n",
       " 'cycl': 15,\n",
       " 'radiat': 2,\n",
       " 'impact': 146,\n",
       " 'axi': 42,\n",
       " 'red': 46,\n",
       " 'shift': 38,\n",
       " 'counter': 8,\n",
       " 'blue': 23,\n",
       " 'conduct': 93,\n",
       " 'quantit': 53,\n",
       " 'between': 584,\n",
       " 'visualis': 31,\n",
       " 'mean': 420,\n",
       " 'other': 604,\n",
       " 'bodi': 32,\n",
       " 'no': 609,\n",
       " 'influenti': 26,\n",
       " 'rank': 154,\n",
       " 'consid': 433,\n",
       " 'second': 150,\n",
       " 'higher': 137,\n",
       " 'ft': 9,\n",
       " 'ed': 160,\n",
       " 'width': 26,\n",
       " 'descript': 102,\n",
       " 'block': 75,\n",
       " 'colour': 16,\n",
       " 'probabl': 157,\n",
       " 'notic': 28,\n",
       " 'discrep': 5,\n",
       " 'absolut': 66,\n",
       " 'irrespect': 2,\n",
       " 'count': 85,\n",
       " 'dead': 4,\n",
       " 'maul': 1,\n",
       " 'dose': 4,\n",
       " 'cur': 15,\n",
       " 'coverag': 143,\n",
       " 'junction': 4,\n",
       " 'word': 148,\n",
       " 'written': 21,\n",
       " 'shown': 239,\n",
       " 'seen': 59,\n",
       " 'exist': 197,\n",
       " 'relationship': 107,\n",
       " 'same': 210,\n",
       " 'across': 73,\n",
       " 'but': 459,\n",
       " 'differenti': 31,\n",
       " 'major': 116,\n",
       " 'prioriti': 11,\n",
       " 'note': 208,\n",
       " 'includ': 523,\n",
       " 'alway': 64,\n",
       " 'find': 250,\n",
       " 'qualiti': 385,\n",
       " 'respons': 208,\n",
       " 'kindr': 1,\n",
       " 'hold': 23,\n",
       " 'littl': 22,\n",
       " 'integr': 152,\n",
       " 'convers': 30,\n",
       " 'more': 836,\n",
       " 'meaning': 87,\n",
       " 'optim': 176,\n",
       " 'relat': 438,\n",
       " 'form': 292,\n",
       " 'valid': 304,\n",
       " 'represent': 294,\n",
       " 'extract': 322,\n",
       " 'taken': 77,\n",
       " 'under': 217,\n",
       " 'well': 373,\n",
       " 'way': 268,\n",
       " 'were': 757,\n",
       " 'either': 72,\n",
       " 'return': 60,\n",
       " 'particularli': 27,\n",
       " 'confirm': 47,\n",
       " 'superior': 19,\n",
       " 'stage': 152,\n",
       " 'step': 188,\n",
       " 'diagnosi': 148,\n",
       " 'multifari': 1,\n",
       " 'object': 505,\n",
       " 'prognosi': 17,\n",
       " 'facilit': 66,\n",
       " 'exchang': 13,\n",
       " 'de': 177,\n",
       " 'anoth': 192,\n",
       " 'prognost': 7,\n",
       " 'while': 313,\n",
       " 'histolog': 2,\n",
       " 'statu': 33,\n",
       " 'morpholog': 5,\n",
       " 'conclus': 119,\n",
       " 'answer': 137,\n",
       " 'bring': 30,\n",
       " 'commun': 186,\n",
       " 'just': 79,\n",
       " 'reason': 561,\n",
       " 'improv': 436,\n",
       " 'rate': 134,\n",
       " 'deduct': 13,\n",
       " 'decid': 54,\n",
       " 'coincid': 2,\n",
       " 'known': 79,\n",
       " 'edg': 45,\n",
       " 'secondari': 20,\n",
       " 'clear': 71,\n",
       " 'substitut': 3,\n",
       " 'discern': 6,\n",
       " 'necess': 15,\n",
       " 'futur': 219,\n",
       " 'refer': 244,\n",
       " 'miller': 81,\n",
       " 'social': 217,\n",
       " 'volum': 174,\n",
       " 'februari': 34,\n",
       " 'page': 100,\n",
       " 'gold': 4,\n",
       " 'abl': 172,\n",
       " 'rigor': 16,\n",
       " 'lent': 19,\n",
       " 'system': 2553,\n",
       " 'small': 88,\n",
       " 'tactic': 18,\n",
       " 'behavior': 284,\n",
       " 'peek': 22,\n",
       " 'insid': 42,\n",
       " 'survey': 268,\n",
       " 'open': 149,\n",
       " 'journal': 268,\n",
       " 'watson': 12,\n",
       " 'heart': 48,\n",
       " 'attack': 326,\n",
       " 'icc': 6,\n",
       " 'th': 237,\n",
       " 'confer': 553,\n",
       " 'springer': 201,\n",
       " 'lang': 5,\n",
       " 'acut': 14,\n",
       " 'ill': 12,\n",
       " 'gandhi': 1,\n",
       " 'powel': 1,\n",
       " 'epidemiolog': 2,\n",
       " 'glob': 1,\n",
       " 'lee': 135,\n",
       " 'unifi': 50,\n",
       " 'annual': 55,\n",
       " 'whi': 350,\n",
       " 'ani': 302,\n",
       " 'pro': 98,\n",
       " 'thirti': 15,\n",
       " 'ar': 21,\n",
       " 'koch': 2,\n",
       " 'inter': 59,\n",
       " 'framework': 358,\n",
       " 'yang': 91,\n",
       " 'biolog': 27,\n",
       " 'option': 34,\n",
       " 'jone': 11,\n",
       " 'publish': 189,\n",
       " 'taylor': 44,\n",
       " 'group': 233,\n",
       " 'geograph': 58,\n",
       " 'ray': 17,\n",
       " 'print': 9,\n",
       " 'onlin': 182,\n",
       " 'homepag': 3,\n",
       " 'task': 461,\n",
       " 'tempor': 179,\n",
       " 'li': 124,\n",
       " 'yi': 23,\n",
       " 'wu': 42,\n",
       " 'cite': 35,\n",
       " 'articl': 383,\n",
       " 'doi': 589,\n",
       " 'link': 71,\n",
       " 'aug': 34,\n",
       " 'sg': 16,\n",
       " 'submit': 16,\n",
       " 'view': 168,\n",
       " 'full': 70,\n",
       " 'action': 307,\n",
       " 'remot': 37,\n",
       " 'sens': 89,\n",
       " 'school': 29,\n",
       " 'earth': 51,\n",
       " 'peke': 8,\n",
       " 'beij': 25,\n",
       " 'china': 33,\n",
       " 'center': 90,\n",
       " 'digit': 70,\n",
       " 'spatial': 122,\n",
       " 'depart': 76,\n",
       " 'geographi': 6,\n",
       " 'illinoi': 1,\n",
       " 'urbana': 2,\n",
       " 'champaign': 1,\n",
       " 'usa': 223,\n",
       " 'info': 5,\n",
       " 'physic': 61,\n",
       " 'central': 35,\n",
       " 'south': 17,\n",
       " 'hunan': 1,\n",
       " 'histori': 41,\n",
       " 'big': 86,\n",
       " 'often': 187,\n",
       " 'aggreg': 57,\n",
       " 'accord': 155,\n",
       " 'receiv': 150,\n",
       " 'octob': 35,\n",
       " 'analyz': 157,\n",
       " 'activ': 284,\n",
       " 'urban': 34,\n",
       " 'environ': 64,\n",
       " 'mani': 303,\n",
       " 'accept': 147,\n",
       " 'juli': 54,\n",
       " 'categor': 89,\n",
       " 'vari': 64,\n",
       " 'deep': 675,\n",
       " 'tensor': 21,\n",
       " 'essenti': 63,\n",
       " 'appar': 19,\n",
       " 'depend': 246,\n",
       " 'varieti': 75,\n",
       " 'imped': 6,\n",
       " 'assess': 320,\n",
       " 'inspir': 30,\n",
       " 'imag': 915,\n",
       " 'compon': 266,\n",
       " 'propos': 541,\n",
       " 'layer': 405,\n",
       " 'wise': 47,\n",
       " 'relev': 283,\n",
       " 'propag': 75,\n",
       " 'organ': 88,\n",
       " 'dimension': 77,\n",
       " 'label': 214,\n",
       " 'typic': 95,\n",
       " 'neural': 763,\n",
       " 'network': 986,\n",
       " 'proce': 6,\n",
       " 'analyt': 25,\n",
       " 'backward': 18,\n",
       " 'taxi': 28,\n",
       " 'trajectori': 18,\n",
       " 'prove': 45,\n",
       " 'attempt': 81,\n",
       " 'discov': 35,\n",
       " 'phenomena': 30,\n",
       " 'chang': 263,\n",
       " 'constantli': 2,\n",
       " 'variat': 121,\n",
       " 'wide': 114,\n",
       " 'repres': 249,\n",
       " 'pattern': 256,\n",
       " 'environment': 16,\n",
       " 'ma': 82,\n",
       " 'preset': 1,\n",
       " 'resolut': 21,\n",
       " 'tag': 17,\n",
       " 'map': 312,\n",
       " 'tessel': 1,\n",
       " 'seri': 58,\n",
       " 'pei': 4,\n",
       " 'contact': 11,\n",
       " 'pku': 3,\n",
       " 'trade': 87,\n",
       " 'panel': 27,\n",
       " 'definit': 119,\n",
       " 'section': 441,\n",
       " 'acquir': 43,\n",
       " 'daili': 21,\n",
       " 'trip': 7,\n",
       " 'weather': 80,\n",
       " 'heterogen': 28,\n",
       " 'pay': 17,\n",
       " 'poorli': 5,\n",
       " 'drain': 10,\n",
       " 'zone': 76,\n",
       " 'previous': 49,\n",
       " 'mention': 92,\n",
       " 'onli': 370,\n",
       " 'benefici': 14,\n",
       " 'guid': 74,\n",
       " 'few': 77,\n",
       " 'topic': 80,\n",
       " 'easi': 36,\n",
       " 'gao': 29,\n",
       " 'independ': 66,\n",
       " 'variabl': 349,\n",
       " 'sensit': 114,\n",
       " 'even': 158,\n",
       " 'necessari': 79,\n",
       " 'daytim': 6,\n",
       " 'valuabl': 35,\n",
       " 'nighttim': 6,\n",
       " 'traffic': 82,\n",
       " 'manag': 237,\n",
       " 'peopl': 129,\n",
       " 'crucial': 69,\n",
       " 'crimin': 27,\n",
       " 'classic': 23,\n",
       " 'densiti': 54,\n",
       " 'cluster': 640,\n",
       " 'procedur': 92,\n",
       " 'special': 37,\n",
       " 'desir': 74,\n",
       " 'variou': 140,\n",
       " 'excel': 24,\n",
       " 'reichstein': 2,\n",
       " 'meanwhil': 18,\n",
       " 'vision': 146,\n",
       " 'address': 153,\n",
       " 'difficulti': 24,\n",
       " 'earlier': 31,\n",
       " 'recurr': 41,\n",
       " 'captur': 119,\n",
       " 'contrast': 128,\n",
       " 'implicit': 26,\n",
       " 'sever': 288,\n",
       " 'bach': 20,\n",
       " 'cam': 169,\n",
       " 'back': 66,\n",
       " 'concept': 296,\n",
       " 'basic': 58,\n",
       " 'principl': 198,\n",
       " 'distinguish': 46,\n",
       " 'pick': 22,\n",
       " 'up': 115,\n",
       " 'weekday': 16,\n",
       " 'weekend': 20,\n",
       " 'holiday': 19,\n",
       " 'spatiotempor': 3,\n",
       " 'dimens': 122,\n",
       " 'verifi': 35,\n",
       " 'four': 150,\n",
       " 'perspect': 133,\n",
       " 'ration': 35,\n",
       " 'compress': 23,\n",
       " 'summar': 59,\n",
       " 'suggest': 196,\n",
       " 'methodolog': 134,\n",
       " 'type': 325,\n",
       " 'check': 97,\n",
       " 'express': 73,\n",
       " 'resembl': 24,\n",
       " 'array': 8,\n",
       " 'previou': 127,\n",
       " 'axe': 80,\n",
       " 'nevertheless': 24,\n",
       " 'cube': 5,\n",
       " 'except': 27,\n",
       " 'divid': 42,\n",
       " 'element': 90,\n",
       " 'fix': 26,\n",
       " 'overview': 100,\n",
       " 'grid': 28,\n",
       " 'color': 53,\n",
       " 'right': 185,\n",
       " 'hand': 139,\n",
       " 'deeper': 34,\n",
       " 'target': 232,\n",
       " 'visual': 472,\n",
       " 'sent': 10,\n",
       " 'accomplish': 20,\n",
       " 'pixel': 96,\n",
       " 'util': 178,\n",
       " 'henc': 84,\n",
       " 'score': 336,\n",
       " 'conserv': 8,\n",
       " 'law': 117,\n",
       " 'eq': 12,\n",
       " 'pm': 6,\n",
       " 'po': 11,\n",
       " 'neuron': 126,\n",
       " 'connect': 132,\n",
       " 'adjac': 5,\n",
       " 're': 129,\n",
       " 'forward': 53,\n",
       " 'pass': 74,\n",
       " 'separ': 82,\n",
       " 'neg': 88,\n",
       " 'posit': 164,\n",
       " 'retain': 18,\n",
       " 'unclear': 5,\n",
       " 'rel': 101,\n",
       " 'locat': 87,\n",
       " 'lu': 19,\n",
       " 'kernel': 45,\n",
       " 'fulli': 83,\n",
       " 'rectangl': 2,\n",
       " 'convolut': 51,\n",
       " 'must': 149,\n",
       " 'divers': 65,\n",
       " 'capabl': 92,\n",
       " 'gather': 37,\n",
       " 'stabl': 14,\n",
       " 'reliabl': 94,\n",
       " 'simpli': 40,\n",
       " 'investig': 152,\n",
       " 'sion': 38,\n",
       " 'detail': 184,\n",
       " 'last': 73,\n",
       " 'correctli': 37,\n",
       " 'subsequ': 47,\n",
       " 'hot': 9,\n",
       " 'rf': 55,\n",
       " 'correct': 103,\n",
       " 'formul': 41,\n",
       " 'dl': 116,\n",
       " 'ko': 5,\n",
       " 'sot': 1,\n",
       " 'total': 75,\n",
       " 'oper': 183,\n",
       " 'hs': 6,\n",
       " 'hr': 7,\n",
       " 'easili': 65,\n",
       " 'worth': 17,\n",
       " 'adjust': 38,\n",
       " 'handl': 29,\n",
       " 'custom': 158,\n",
       " 'interv': 25,\n",
       " 'area': 243,\n",
       " 'rang': 155,\n",
       " 'statist': 257,\n",
       " 'pss': 1,\n",
       " 'conveni': 9,\n",
       " 'irregular': 9,\n",
       " 'graph': 106,\n",
       " 'raster': 1,\n",
       " 'directli': 66,\n",
       " 'resiz': 3,\n",
       " 'requir': 354,\n",
       " 'band': 34,\n",
       " 'alreadi': 60,\n",
       " 'spectral': 11,\n",
       " 'unchang': 10,\n",
       " 'least': 57,\n",
       " 'season': 3,\n",
       " 'event': 126,\n",
       " 'polici': 56,\n",
       " 'subway': 7,\n",
       " 'extrem': 26,\n",
       " 'slice': 6,\n",
       " 'land': 18,\n",
       " 'congest': 6,\n",
       " 'suburb': 1,\n",
       " 'down': 59,\n",
       " 'town': 3,\n",
       " 'third': 56,\n",
       " 'long': 89,\n",
       " 'short': 44,\n",
       " 'memori': 37,\n",
       " 'net': 85,\n",
       " 'order': 231,\n",
       " 'enough': 43,\n",
       " 'guarante': 35,\n",
       " 'situat': 57,\n",
       " 'introduc': 141,\n",
       " 'normal': 214,\n",
       " 'recommend': 186,\n",
       " 'taxicab': 2,\n",
       " 'transport': 64,\n",
       " 'mode': 42,\n",
       " 'proxi': 28,\n",
       " 'citizen': 7,\n",
       " 'simpl': 139,\n",
       " 'gp': 7,\n",
       " 'sensor': 71,\n",
       " 'moreov': 120,\n",
       " 'movement': 44,\n",
       " 'destin': 7,\n",
       " 'od': 14,\n",
       " 'raw': 21,\n",
       " 'travel': 11,\n",
       " 'demand': 63,\n",
       " 'resid': 5,\n",
       " 'dispatch': 1,\n",
       " 'design': 359,\n",
       " 'choos': 50,\n",
       " 'approxim': 136,\n",
       " 'ring': 51,\n",
       " 'road': 26,\n",
       " 'km': 11,\n",
       " 'whole': 61,\n",
       " 'compani': 102,\n",
       " 'mask': 31,\n",
       " 'id': 75,\n",
       " 'coordin': 39,\n",
       " 'utc': 40,\n",
       " 'ge': 13,\n",
       " 'ic': 14,\n",
       " 'sth': 3,\n",
       " 'mark': 31,\n",
       " 'empti': 4,\n",
       " 'occupi': 5,\n",
       " 'moment': 7,\n",
       " 'filter': 92,\n",
       " 'criteria': 120,\n",
       " 'speed': 35,\n",
       " 'less': 93,\n",
       " 'distanc': 83,\n",
       " 'meter': 7,\n",
       " 'half': 15,\n",
       " 'hour': 19,\n",
       " 'day': 79,\n",
       " 'drawn': 13,\n",
       " 'period': 29,\n",
       " 'examin': 68,\n",
       " 'build': 245,\n",
       " 'encod': 34,\n",
       " 'chines': 17,\n",
       " 'schedul': 8,\n",
       " 'stride': 8,\n",
       " 'reduc': 101,\n",
       " 'max': 164,\n",
       " 'pool': 71,\n",
       " 'program': 227,\n",
       " 'cover': 70,\n",
       " 'miss': 28,\n",
       " 'randomli': 31,\n",
       " ...}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_tok_dic = create_unig(stemmed_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "911d7537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>TF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>10140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comparison</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>explan</td>\n",
       "      <td>3531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>given</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>by</td>\n",
       "      <td>2360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7614</th>\n",
       "      <td>archer</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7615</th>\n",
       "      <td>studio</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7616</th>\n",
       "      <td>saint</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7617</th>\n",
       "      <td>interlaken</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7618</th>\n",
       "      <td>strain</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7619 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Token     TF\n",
       "0              a  10140\n",
       "1     comparison    148\n",
       "2         explan   3531\n",
       "3          given    340\n",
       "4             by   2360\n",
       "...          ...    ...\n",
       "7614      archer      1\n",
       "7615      studio      1\n",
       "7616       saint      1\n",
       "7617  interlaken      1\n",
       "7618      strain      1\n",
       "\n",
       "[7619 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_tok_df = pd.DataFrame({\"Token\" : stemmed_tok_dic.keys(), \"TF\" : stemmed_tok_dic.values()})\n",
    "stemmed_tok_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "32c75543",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_tok_df_df = df(stemmed_tok_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d43f2f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_tok_df['df'] = stemmed_tok_df_df.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "814cb348",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_tok_df[\"idf\"] = np.log((50/stemmed_tok_df[\"df\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b56ba72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>TF</th>\n",
       "      <th>df</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>10140</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comparison</td>\n",
       "      <td>148</td>\n",
       "      <td>37</td>\n",
       "      <td>0.301105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>explan</td>\n",
       "      <td>3531</td>\n",
       "      <td>48</td>\n",
       "      <td>0.040822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>given</td>\n",
       "      <td>340</td>\n",
       "      <td>45</td>\n",
       "      <td>0.105361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>by</td>\n",
       "      <td>2360</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7614</th>\n",
       "      <td>archer</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>0.328504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7615</th>\n",
       "      <td>studio</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7616</th>\n",
       "      <td>saint</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3.912023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7617</th>\n",
       "      <td>interlaken</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7618</th>\n",
       "      <td>strain</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.579818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7619 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Token     TF  df       idf\n",
       "0              a  10140  50  0.000000\n",
       "1     comparison    148  37  0.301105\n",
       "2         explan   3531  48  0.040822\n",
       "3          given    340  45  0.105361\n",
       "4             by   2360  50  0.000000\n",
       "...          ...    ...  ..       ...\n",
       "7614      archer      1  36  0.328504\n",
       "7615      studio      1   0       inf\n",
       "7616       saint      1   1  3.912023\n",
       "7617  interlaken      1   0       inf\n",
       "7618      strain      1  28  0.579818\n",
       "\n",
       "[7619 rows x 4 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_tok_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54f772ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token</th>\n",
       "      <th>TF</th>\n",
       "      <th>df</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>10140</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>in</td>\n",
       "      <td>9283</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>is</td>\n",
       "      <td>4941</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>model</td>\n",
       "      <td>4217</td>\n",
       "      <td>49</td>\n",
       "      <td>0.020203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>on</td>\n",
       "      <td>3711</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>explan</td>\n",
       "      <td>3531</td>\n",
       "      <td>48</td>\n",
       "      <td>0.040822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>as</td>\n",
       "      <td>3154</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>are</td>\n",
       "      <td>2874</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>be</td>\n",
       "      <td>2617</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>system</td>\n",
       "      <td>2553</td>\n",
       "      <td>49</td>\n",
       "      <td>0.020203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Token     TF  df       idf\n",
       "0         a  10140  50  0.000000\n",
       "49       in   9283  50  0.000000\n",
       "55       is   4941  50  0.000000\n",
       "66    model   4217  49  0.020203\n",
       "9        on   3711  50  0.000000\n",
       "2    explan   3531  48  0.040822\n",
       "33       as   3154  50  0.000000\n",
       "102     are   2874  50  0.000000\n",
       "194      be   2617  50  0.000000\n",
       "599  system   2553  49  0.020203"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_10 = stemmed_tok_df.sort_values(by=['TF'], ascending=False)[0:10]\n",
    "stem_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66c35e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'in', 'is', 'model', 'on', 'explan', 'as', 'are', 'be', 'system']\n",
      "['a', 'in', 'is', 'on', 'as', 'are', 'model', 'be', 'by', 'an']\n",
      "['A', 'model', 'In', 'data', 'explanations', 'explanation', 'based', 'models', 'learning', 'AI']\n",
      "['on', 'A', 'by', 'Explainable', 'Intelligence', 'Artificial', 'Explanations', 'Methods', 'Given', 'Comparison']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_10_l = list(stem_10[\"Token\"])\n",
    "caseFol_10_l = list(caseFol_10[\"Token\"])\n",
    "stop_words_l = list(stopW_10[\"Token\"])\n",
    "token_10_l = list(token_10[\"Token\"])\n",
    "\n",
    "print(stem_10_l)\n",
    "print(caseFol_10_l)\n",
    "print(stop_words_l)\n",
    "print(token_10_l)\n",
    "\n",
    "i1 = set(stem_10_l).intersection(set(caseFol_10_l))\n",
    "i2 = i1.intersection(set(stop_words_l))\n",
    "i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6b0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b471910d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3da535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e5650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f43592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
